{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RedCoast","text":"<p>Red Coast (redco) is a lightweight and user-friendly tool designed to automate distributed training and inference for large models while simplifying the ML pipeline development process without necessitating MLSys expertise from users.</p> <p>RedCoast supports Large Models + Complex Algorithms, in a lightweight and user-friendly way: </p> <ul> <li>Large Models beyond Transformers, e.g, Stable Diffusion, etc.</li> <li>Complex algorithms beyond cross entropy, e.g., Meta Learning, DP Training, etc.</li> </ul> <p>With RedCoast, to define a ML pipeline, only three functions are needed:</p> <ul> <li>Collate function: convert raw data into model inputs (e.g., text tokenization);</li> <li>Loss function: execute the model and compute loss (e.g., cross-entropy);</li> <li>Predict function: run the model and deliver outcomes (e.g., beam search).</li> </ul> <p>Redco automates all the remaining of pipeline execution such as data and model parallelism, multi-host related processing, distributed checkpointing, randomness controlling, logging, etc.</p> <p></p>"},{"location":"docs/deployer/","title":"Deployer","text":""},{"location":"docs/deployer/#redco.deployers.deployer.Deployer","title":"<code>Deployer</code>","text":"<p>Handles low-level operations to support Trainer and Predictor,     e.g., automatic data/model parallelism, distributed checkpointing,     data processing, logging, randomness controlling, etc.</p> <p>Attributes:</p> Name Type Description <code>workdir</code> <code>str</code> <p>Working directory for saving checkpoints and logs.</p> <code>mesh</code> <code>jax Mesh</code> <p>Mesh used for model sharding.</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>class Deployer:\n    \"\"\" Handles low-level operations to support Trainer and Predictor,\n        e.g., automatic data/model parallelism, distributed checkpointing,\n        data processing, logging, randomness controlling, etc.\n\n    Attributes:\n        workdir (str): Working directory for saving checkpoints and logs.\n        mesh (jax Mesh): Mesh used for model sharding.\n    \"\"\"\n    def __init__(self,\n                 jax_seed,\n                 n_model_shards=1,\n                 verbose=True,\n                 workdir=None,\n                 n_processes=None,\n                 host0_address=None,\n                 host0_port=None,\n                 process_id=None,\n                 n_local_devices=None,\n                 run_tensorboard=False,\n                 wandb_init_kwargs=None):\n        \"\"\" Initializes a Deployer.\n\n        Args:\n            jax_seed (int): Seed for random number generation.\n            n_model_shards (int): Number of shards for running large model.\n            verbose (bool): Whether to enable verbose logging.\n            workdir (str):  Directory for saving logs and checkpoints.\n            n_processes (int):  For multi-host, number of processes/nodes.\n            host0_address (str):  For multi-host, address of the host0.\n            host0_port (int): For multi-host, port of the host0.\n            process_id (int): For multi-host, index of the current process.\n            n_local_devices (int): For multi-host, number of local devices.\n            run_tensorboard (bool):  Whether to enable TensorBoard logging.\n            wandb_init_kwargs (dict): wandb.init arguments if using wandb.\n        \"\"\"\n        if n_processes is None:\n            if 'SLURM_JOB_NUM_NODES' in os.environ:\n                n_processes = int(os.environ['SLURM_JOB_NUM_NODES'])\n                process_id = int(os.environ['SLURM_NODEID'])\n            else:\n                n_processes = 1\n\n        if n_processes &gt; 1:\n            local_device_ids = None if n_local_devices is None \\\n                else list(range(n_local_devices))\n\n            if host0_port is None:\n                host0_port = DEFAULT_HOST0_PORT\n\n            jax.distributed.initialize(\n                coordinator_address=f'{host0_address}:{host0_port}',\n                num_processes=n_processes,\n                process_id=process_id,\n                local_device_ids=local_device_ids)\n\n        if workdir is not None:\n            os.makedirs(workdir, exist_ok=True)\n\n        self._verbose = verbose\n        self._workdir = workdir\n        self._logger = get_logger(verbose=verbose, workdir=workdir)\n\n        if wandb_init_kwargs is not None and jax.process_index() == 0:\n            import wandb\n            wandb.init(**wandb_init_kwargs)\n            self._wandb_log_fn = wandb.log\n        else:\n            self._wandb_log_fn = None\n\n        if run_tensorboard and jax.process_index() == 0:\n            from flax.metrics import tensorboard\n            self._summary_writer = tensorboard.SummaryWriter(workdir)\n        else:\n            self._summary_writer = None\n\n        self.log_info(\n            f'Local Devices: {jax.local_device_count()} / {jax.device_count()}')\n\n        self._rng = jax.random.PRNGKey(seed=jax_seed)\n        self._mesh = get_mesh(n_model_shards=n_model_shards)\n        self._checkpointer = ocp.PyTreeCheckpointer()\n\n    def get_local_global_micro_batch_size(self, per_device_batch_size):\n        \"\"\"Get local/global micro batch sizes based on per-device batch size.\"\"\"\n        if self._mesh is None:\n            local_micro_batch_size = \\\n                per_device_batch_size * jax.local_device_count()\n            global_micro_batch_size = \\\n                local_micro_batch_size * jax.process_count()\n        else:\n            global_micro_batch_size = local_micro_batch_size = \\\n                per_device_batch_size * self._mesh.shape['dp']\n\n        return local_micro_batch_size, global_micro_batch_size\n\n    def get_accumulate_grad_batches(\n            self, global_batch_size, per_device_batch_size):\n        \"\"\"Calculates the number of gradient accumulation batches.\"\"\"\n        _, global_micro_batch_size = self.get_local_global_micro_batch_size(\n            per_device_batch_size=per_device_batch_size)\n        assert global_batch_size % global_micro_batch_size == 0\n        accumulate_grad_batches = global_batch_size // global_micro_batch_size\n\n        return accumulate_grad_batches\n\n    def get_model_input_batches(self,\n                                examples,\n                                per_device_batch_size,\n                                collate_fn,\n                                shuffle,\n                                shuffle_rng,\n                                desc,\n                                is_train=False,\n                                accumulate_grad_batches=None):\n        \"\"\"Prepares model input batches from examples.\n\n        Args:\n            examples (list): List of input examples.\n            per_device_batch_size (int): Batch size per device.\n            collate_fn (Callable): Function to collate the examples.\n            shuffle (bool): Whether to shuffle the examples.\n            shuffle_rng (`jax.numpy.Array`): RNG for randomness of shuffling.\n            desc (str): Description in the progress bar.\n            is_train (bool): Whether the data is for training.\n            accumulate_grad_batches (int): gradient accumulation batches.\n\n        Returns:\n            (generator): A python generator of batched model inputs.\n        \"\"\"\n        local_micro_batch_size, global_micro_batch_size = \\\n            self.get_local_global_micro_batch_size(\n                per_device_batch_size=per_device_batch_size)\n\n        examples = get_host_examples(\n            examples=examples,\n            global_micro_batch_size=global_micro_batch_size,\n            shuffle=shuffle,\n            shuffle_rng=shuffle_rng,\n            mesh=self._mesh)\n\n        if not is_train:\n            desc = f'{desc} (global_batch_size = {global_micro_batch_size})'\n        elif accumulate_grad_batches is None:\n            desc = \\\n                f'{desc} (global_micro_batch_size = {global_micro_batch_size})'\n        else:\n            desc = (f'{desc} ('\n                    f'global_micro_batch_size = {global_micro_batch_size}, '\n                    f'accumulate_grad_batches = {accumulate_grad_batches})')\n\n        return get_data_batches(\n            examples=examples,\n            batch_size=local_micro_batch_size,\n            collate_fn=collate_fn,\n            mesh=self._mesh,\n            desc=desc,\n            verbose=self._verbose)\n\n    def get_lr_schedule_fn(self,\n                           train_size,\n                           per_device_batch_size,\n                           n_epochs,\n                           learning_rate,\n                           schedule_type='linear',\n                           warmup_ratio=0.,\n                           warmup_steps=None,\n                           init_learning_rate=0.,\n                           end_learning_rate=0.):\n        \"\"\"Creates a learning rate schedule function.\n\n        Args:\n            train_size (int): Number of training examples per epoch.\n            per_device_batch_size (int): Batch size per device.\n            n_epochs (int): Number of epochs.\n            learning_rate (float): Peak learning rate.\n            schedule_type (str): Type of lr schedule, \"linear\" or \"cosine\".\n            warmup_ratio (float): Ratio of lr warmup.\n            warmup_steps (int): Number of warmup steps.\n            init_learning_rate (float): Initial learning rate before warmup.\n            end_learning_rate (float): End learning rate for the schedule.\n\n        Returns:\n            (Callable): A lr schedule function, step -&gt; learning rate.\n        \"\"\"\n        _, global_micro_batch_size = self.get_local_global_micro_batch_size(\n            per_device_batch_size=per_device_batch_size)\n        total_train_steps = n_epochs * (train_size // global_micro_batch_size)\n\n        if warmup_steps is None:\n            warmup_steps = int(total_train_steps * warmup_ratio)\n\n        return get_lr_schedule_fn(\n            schedule_type=schedule_type,\n            total_train_steps=total_train_steps,\n            warmup_steps=warmup_steps,\n            init_learning_rate=init_learning_rate,\n            learning_rate=learning_rate,\n            end_learning_rate=end_learning_rate)\n\n    def get_sharding_rules(self, params_shape_or_params):\n        \"\"\"Get sharding rules based on the parameter shapes.\"\"\"\n        if self._mesh is None:\n            return None\n        else:\n            sharding_rules = get_sharding_rules(\n                params_shape_or_params=params_shape_or_params,\n                n_model_shards=self._mesh.shape['mp'])\n            return sharding_rules\n\n    def get_params_spec(self, params_shape_or_params, params_sharding_rules):\n        \"\"\"Generates parameter specs based on sharding rules.\"\"\"\n        return get_params_spec(\n            params_shape_or_params=params_shape_or_params,\n            params_sharding_rules=params_sharding_rules)\n\n    def get_opt_state_spec(\n            self, params_shape_or_params, params_spec, optimizer):\n        \"\"\"Get optimizer state specs\"\"\"\n        return get_opt_state_spec(\n            params_shape_or_params=params_shape_or_params,\n            params_spec=params_spec,\n            optimizer=optimizer)\n\n    def shard_params(self, params, params_spec, desc='params'):\n        \"\"\"Distributes parameters to all devices based on the provided specs.\"\"\"\n        self.log_info(info=f'Sharding {desc} ...')\n        return shard_params(\n            mesh=self._mesh, params=params, params_spec=params_spec)\n\n    def run_model_step(self, step_fn, input_args):\n        \"\"\"Executes a model step function with the provided inputs.\"\"\"\n        if self._mesh is None:\n            return step_fn(*input_args)\n        else:\n            with self._mesh:\n                return step_fn(*input_args)\n\n    def gen_rng(self):\n        \"\"\"Get a new random number generator key and update the random state.\"\"\"\n        self._rng, new_rng = jax.random.split(self._rng)\n        return new_rng\n\n    def gen_model_step_rng(self):\n        \"\"\"Get a new random number generator key for distributed model step and\n        update the random state.\n        \"\"\"\n        rng = self.gen_rng()\n        if self.mesh is None:\n            rng = jax.random.split(\n                rng, num=jax.process_count())[jax.process_index()]\n            rng = shard_prng_key(rng)\n        return rng\n\n    def log_info(self, info, title=None, step=None):\n        \"\"\"Logs a messages\"\"\"\n        log_info(\n            info=info,\n            title=title,\n            logger=self._logger,\n            summary_writer=self._summary_writer,\n            step=step)\n\n    def log_metrics(self, metrics, step):\n        \"\"\"Logs metrics to TensorBoard and Weights and Biases (wandb).\"\"\"\n        if self._summary_writer is not None:\n            for metric_name, value in metrics.items():\n                self._summary_writer.scalar(metric_name, value, step=step)\n\n        if self._wandb_log_fn is not None:\n            self._wandb_log_fn(metrics, step)\n\n    def save_outputs(self, outputs, desc, step):\n        \"\"\"Saves model outputs to workdir.\"\"\"\n        if self._workdir is not None and jax.process_index() == 0:\n            save_outputs(\n                workdir=self._workdir,\n                outputs=outputs,\n                desc=desc,\n                step=step,\n                logger=self._logger,\n                summary_writer=self._summary_writer)\n\n    def save_ckpt(\n            self, ckpt_dir, params, opt_state=None, float_dtype=None, **kwargs):\n        \"\"\"Saves a checkpoint to the specified directory.\n\n        Args:\n            ckpt_dir (str): Directory to save the checkpoint.\n            params (dict): Model parameters.\n            opt_state (dict): Optimizer state.\n            float_dtype (`jax.numpy.dtype`): Dtype for floating point numbers.\n            **kwargs (dict): Additional information to be saved into\n                info.json, e.g., current training step, epoch index, etc.\n        \"\"\"\n        ckpt_dir = os.path.abspath(ckpt_dir)\n        self.log_info(f'Saving ckpt to {ckpt_dir} ...')\n        save_ckpt(\n            ckpt_dir=ckpt_dir,\n            checkpointer=self._checkpointer,\n            params=params,\n            opt_state=opt_state,\n            float_dtype=float_dtype,\n            rng=self._rng,\n            **kwargs)\n        self.log_info(f'Ckpt saved into {ckpt_dir}')\n\n    def load_params_shape(self, ckpt_dir):\n        \"\"\"Loads the shape of the parameters from a checkpoint.\"\"\"\n        return load_params_shape(ckpt_dir=ckpt_dir)\n\n    def load_ckpt(self,\n                  ckpt_dir,\n                  params_sharding_rules=None,\n                  optimizer=None,\n                  float_dtype=None,\n                  load_params=True,\n                  load_opt_state=True,\n                  update_rng=False):\n        \"\"\"Loads a checkpoint from the specified directory.\n\n        Args:\n            ckpt_dir (str): Directory of the checkpoint.\n            params_sharding_rules (list[tuple]): Sharding rules for parameters.\n            optimizer (optax optimizer): Optimizer for loading opt_state.\n            float_dtype (`jax.numpy.dtype`): Dtype for floating point numbers.\n            load_params (bool): Whether to load the parameters.\n            load_opt_state (bool): Whether to load the optimizer state.\n            update_rng (bool): if updating the random state of the deployer.\n\n        Returns:\n            (tuple): A tuple with the loaded checkpoint (in a dict with\n                `\"params\"` and `\"opt_state\"`) and additional information (in a\n                dict, usually including `\"steps\"`, `\"epoch_idx\"`, and `\"rng\"`).\n        \"\"\"\n        ckpt_dir = os.path.abspath(ckpt_dir)\n        self.log_info(f'Loading ckpt from {ckpt_dir} ...')\n\n        params_shape = self.load_params_shape(ckpt_dir=ckpt_dir)\n\n        specs = {}\n        if self._mesh is not None:\n            if params_sharding_rules is None:\n                params_sharding_rules = self.get_sharding_rules(\n                    params_shape_or_params=params_shape)\n\n            specs['params'] = self.get_params_spec(\n                params_shape_or_params=params_shape,\n                params_sharding_rules=params_sharding_rules)\n            if optimizer is not None:\n                specs['opt_state'] = self.get_opt_state_spec(\n                    params_shape_or_params=params_shape,\n                    params_spec=specs['params'],\n                    optimizer=optimizer)\n\n        ckpt, info = load_ckpt(\n            ckpt_dir=ckpt_dir,\n            checkpointer=self._checkpointer,\n            params_shape_or_params=params_shape,\n            optimizer=optimizer,\n            float_dtype=float_dtype,\n            mesh=self._mesh,\n            specs=specs,\n            load_params=load_params,\n            load_opt_state=load_opt_state)\n\n        for key, value in info.items():\n            if not update_rng and key == 'rng':\n                continue\n            self.log_info(f'{ckpt_dir}::{key} = {value}')\n\n        if update_rng:\n            self._rng = info['rng']\n            self.log_info(f'rng updated to {self._rng} (by {ckpt_dir})')\n\n        return ckpt, info\n\n    def load_last_ckpt(self,\n                       optimizer=None,\n                       params_sharding_rules=None,\n                       float_dtype=None,\n                       load_params=True,\n                       load_opt_state=True,\n                       update_rng=True):\n        \"\"\"Loads the last checkpoint from the work directory (self.workdir).\n        See load_ckpt() for the explanation of arguments.\n        \"\"\"\n        try:\n            last_ckpt_name = open(\n                f'{self._workdir}/ckpts/last_ckpt.txt').read().strip()\n        except:\n            self.log_info(\n                f'{self._workdir}/ckpts/last_ckpt.txt not found. '\n                f'no ckpt loaded.')\n            return None, None\n\n        return self.load_ckpt(\n            ckpt_dir=f'{self._workdir}/ckpts/{last_ckpt_name}',\n            optimizer=optimizer,\n            float_dtype=float_dtype,\n            params_sharding_rules=params_sharding_rules,\n            load_params=load_params,\n            load_opt_state=load_opt_state,\n            update_rng=update_rng)\n\n    @property\n    def mesh(self):\n        \"\"\"Returns the mesh for model sharding\"\"\"\n        return self._mesh\n\n    @property\n    def workdir(self):\n        \"\"\"Returns the work directory.\"\"\"\n        return self._workdir\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.mesh","title":"<code>mesh</code>  <code>property</code>","text":"<p>Returns the mesh for model sharding</p>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.workdir","title":"<code>workdir</code>  <code>property</code>","text":"<p>Returns the work directory.</p>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.__init__","title":"<code>__init__(jax_seed, n_model_shards=1, verbose=True, workdir=None, n_processes=None, host0_address=None, host0_port=None, process_id=None, n_local_devices=None, run_tensorboard=False, wandb_init_kwargs=None)</code>","text":"<p>Initializes a Deployer.</p> <p>Parameters:</p> Name Type Description Default <code>jax_seed</code> <code>int</code> <p>Seed for random number generation.</p> required <code>n_model_shards</code> <code>int</code> <p>Number of shards for running large model.</p> <code>1</code> <code>verbose</code> <code>bool</code> <p>Whether to enable verbose logging.</p> <code>True</code> <code>workdir</code> <code>str</code> <p>Directory for saving logs and checkpoints.</p> <code>None</code> <code>n_processes</code> <code>int</code> <p>For multi-host, number of processes/nodes.</p> <code>None</code> <code>host0_address</code> <code>str</code> <p>For multi-host, address of the host0.</p> <code>None</code> <code>host0_port</code> <code>int</code> <p>For multi-host, port of the host0.</p> <code>None</code> <code>process_id</code> <code>int</code> <p>For multi-host, index of the current process.</p> <code>None</code> <code>n_local_devices</code> <code>int</code> <p>For multi-host, number of local devices.</p> <code>None</code> <code>run_tensorboard</code> <code>bool</code> <p>Whether to enable TensorBoard logging.</p> <code>False</code> <code>wandb_init_kwargs</code> <code>dict</code> <p>wandb.init arguments if using wandb.</p> <code>None</code> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def __init__(self,\n             jax_seed,\n             n_model_shards=1,\n             verbose=True,\n             workdir=None,\n             n_processes=None,\n             host0_address=None,\n             host0_port=None,\n             process_id=None,\n             n_local_devices=None,\n             run_tensorboard=False,\n             wandb_init_kwargs=None):\n    \"\"\" Initializes a Deployer.\n\n    Args:\n        jax_seed (int): Seed for random number generation.\n        n_model_shards (int): Number of shards for running large model.\n        verbose (bool): Whether to enable verbose logging.\n        workdir (str):  Directory for saving logs and checkpoints.\n        n_processes (int):  For multi-host, number of processes/nodes.\n        host0_address (str):  For multi-host, address of the host0.\n        host0_port (int): For multi-host, port of the host0.\n        process_id (int): For multi-host, index of the current process.\n        n_local_devices (int): For multi-host, number of local devices.\n        run_tensorboard (bool):  Whether to enable TensorBoard logging.\n        wandb_init_kwargs (dict): wandb.init arguments if using wandb.\n    \"\"\"\n    if n_processes is None:\n        if 'SLURM_JOB_NUM_NODES' in os.environ:\n            n_processes = int(os.environ['SLURM_JOB_NUM_NODES'])\n            process_id = int(os.environ['SLURM_NODEID'])\n        else:\n            n_processes = 1\n\n    if n_processes &gt; 1:\n        local_device_ids = None if n_local_devices is None \\\n            else list(range(n_local_devices))\n\n        if host0_port is None:\n            host0_port = DEFAULT_HOST0_PORT\n\n        jax.distributed.initialize(\n            coordinator_address=f'{host0_address}:{host0_port}',\n            num_processes=n_processes,\n            process_id=process_id,\n            local_device_ids=local_device_ids)\n\n    if workdir is not None:\n        os.makedirs(workdir, exist_ok=True)\n\n    self._verbose = verbose\n    self._workdir = workdir\n    self._logger = get_logger(verbose=verbose, workdir=workdir)\n\n    if wandb_init_kwargs is not None and jax.process_index() == 0:\n        import wandb\n        wandb.init(**wandb_init_kwargs)\n        self._wandb_log_fn = wandb.log\n    else:\n        self._wandb_log_fn = None\n\n    if run_tensorboard and jax.process_index() == 0:\n        from flax.metrics import tensorboard\n        self._summary_writer = tensorboard.SummaryWriter(workdir)\n    else:\n        self._summary_writer = None\n\n    self.log_info(\n        f'Local Devices: {jax.local_device_count()} / {jax.device_count()}')\n\n    self._rng = jax.random.PRNGKey(seed=jax_seed)\n    self._mesh = get_mesh(n_model_shards=n_model_shards)\n    self._checkpointer = ocp.PyTreeCheckpointer()\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.gen_model_step_rng","title":"<code>gen_model_step_rng()</code>","text":"<p>Get a new random number generator key for distributed model step and update the random state.</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def gen_model_step_rng(self):\n    \"\"\"Get a new random number generator key for distributed model step and\n    update the random state.\n    \"\"\"\n    rng = self.gen_rng()\n    if self.mesh is None:\n        rng = jax.random.split(\n            rng, num=jax.process_count())[jax.process_index()]\n        rng = shard_prng_key(rng)\n    return rng\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.gen_rng","title":"<code>gen_rng()</code>","text":"<p>Get a new random number generator key and update the random state.</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def gen_rng(self):\n    \"\"\"Get a new random number generator key and update the random state.\"\"\"\n    self._rng, new_rng = jax.random.split(self._rng)\n    return new_rng\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.get_accumulate_grad_batches","title":"<code>get_accumulate_grad_batches(global_batch_size, per_device_batch_size)</code>","text":"<p>Calculates the number of gradient accumulation batches.</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def get_accumulate_grad_batches(\n        self, global_batch_size, per_device_batch_size):\n    \"\"\"Calculates the number of gradient accumulation batches.\"\"\"\n    _, global_micro_batch_size = self.get_local_global_micro_batch_size(\n        per_device_batch_size=per_device_batch_size)\n    assert global_batch_size % global_micro_batch_size == 0\n    accumulate_grad_batches = global_batch_size // global_micro_batch_size\n\n    return accumulate_grad_batches\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.get_local_global_micro_batch_size","title":"<code>get_local_global_micro_batch_size(per_device_batch_size)</code>","text":"<p>Get local/global micro batch sizes based on per-device batch size.</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def get_local_global_micro_batch_size(self, per_device_batch_size):\n    \"\"\"Get local/global micro batch sizes based on per-device batch size.\"\"\"\n    if self._mesh is None:\n        local_micro_batch_size = \\\n            per_device_batch_size * jax.local_device_count()\n        global_micro_batch_size = \\\n            local_micro_batch_size * jax.process_count()\n    else:\n        global_micro_batch_size = local_micro_batch_size = \\\n            per_device_batch_size * self._mesh.shape['dp']\n\n    return local_micro_batch_size, global_micro_batch_size\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.get_lr_schedule_fn","title":"<code>get_lr_schedule_fn(train_size, per_device_batch_size, n_epochs, learning_rate, schedule_type='linear', warmup_ratio=0.0, warmup_steps=None, init_learning_rate=0.0, end_learning_rate=0.0)</code>","text":"<p>Creates a learning rate schedule function.</p> <p>Parameters:</p> Name Type Description Default <code>train_size</code> <code>int</code> <p>Number of training examples per epoch.</p> required <code>per_device_batch_size</code> <code>int</code> <p>Batch size per device.</p> required <code>n_epochs</code> <code>int</code> <p>Number of epochs.</p> required <code>learning_rate</code> <code>float</code> <p>Peak learning rate.</p> required <code>schedule_type</code> <code>str</code> <p>Type of lr schedule, \"linear\" or \"cosine\".</p> <code>'linear'</code> <code>warmup_ratio</code> <code>float</code> <p>Ratio of lr warmup.</p> <code>0.0</code> <code>warmup_steps</code> <code>int</code> <p>Number of warmup steps.</p> <code>None</code> <code>init_learning_rate</code> <code>float</code> <p>Initial learning rate before warmup.</p> <code>0.0</code> <code>end_learning_rate</code> <code>float</code> <p>End learning rate for the schedule.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A lr schedule function, step -&gt; learning rate.</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def get_lr_schedule_fn(self,\n                       train_size,\n                       per_device_batch_size,\n                       n_epochs,\n                       learning_rate,\n                       schedule_type='linear',\n                       warmup_ratio=0.,\n                       warmup_steps=None,\n                       init_learning_rate=0.,\n                       end_learning_rate=0.):\n    \"\"\"Creates a learning rate schedule function.\n\n    Args:\n        train_size (int): Number of training examples per epoch.\n        per_device_batch_size (int): Batch size per device.\n        n_epochs (int): Number of epochs.\n        learning_rate (float): Peak learning rate.\n        schedule_type (str): Type of lr schedule, \"linear\" or \"cosine\".\n        warmup_ratio (float): Ratio of lr warmup.\n        warmup_steps (int): Number of warmup steps.\n        init_learning_rate (float): Initial learning rate before warmup.\n        end_learning_rate (float): End learning rate for the schedule.\n\n    Returns:\n        (Callable): A lr schedule function, step -&gt; learning rate.\n    \"\"\"\n    _, global_micro_batch_size = self.get_local_global_micro_batch_size(\n        per_device_batch_size=per_device_batch_size)\n    total_train_steps = n_epochs * (train_size // global_micro_batch_size)\n\n    if warmup_steps is None:\n        warmup_steps = int(total_train_steps * warmup_ratio)\n\n    return get_lr_schedule_fn(\n        schedule_type=schedule_type,\n        total_train_steps=total_train_steps,\n        warmup_steps=warmup_steps,\n        init_learning_rate=init_learning_rate,\n        learning_rate=learning_rate,\n        end_learning_rate=end_learning_rate)\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.get_model_input_batches","title":"<code>get_model_input_batches(examples, per_device_batch_size, collate_fn, shuffle, shuffle_rng, desc, is_train=False, accumulate_grad_batches=None)</code>","text":"<p>Prepares model input batches from examples.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>list</code> <p>List of input examples.</p> required <code>per_device_batch_size</code> <code>int</code> <p>Batch size per device.</p> required <code>collate_fn</code> <code>Callable</code> <p>Function to collate the examples.</p> required <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the examples.</p> required <code>shuffle_rng</code> <code>`jax.numpy.Array`</code> <p>RNG for randomness of shuffling.</p> required <code>desc</code> <code>str</code> <p>Description in the progress bar.</p> required <code>is_train</code> <code>bool</code> <p>Whether the data is for training.</p> <code>False</code> <code>accumulate_grad_batches</code> <code>int</code> <p>gradient accumulation batches.</p> <code>None</code> <p>Returns:</p> Type Description <code>generator</code> <p>A python generator of batched model inputs.</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def get_model_input_batches(self,\n                            examples,\n                            per_device_batch_size,\n                            collate_fn,\n                            shuffle,\n                            shuffle_rng,\n                            desc,\n                            is_train=False,\n                            accumulate_grad_batches=None):\n    \"\"\"Prepares model input batches from examples.\n\n    Args:\n        examples (list): List of input examples.\n        per_device_batch_size (int): Batch size per device.\n        collate_fn (Callable): Function to collate the examples.\n        shuffle (bool): Whether to shuffle the examples.\n        shuffle_rng (`jax.numpy.Array`): RNG for randomness of shuffling.\n        desc (str): Description in the progress bar.\n        is_train (bool): Whether the data is for training.\n        accumulate_grad_batches (int): gradient accumulation batches.\n\n    Returns:\n        (generator): A python generator of batched model inputs.\n    \"\"\"\n    local_micro_batch_size, global_micro_batch_size = \\\n        self.get_local_global_micro_batch_size(\n            per_device_batch_size=per_device_batch_size)\n\n    examples = get_host_examples(\n        examples=examples,\n        global_micro_batch_size=global_micro_batch_size,\n        shuffle=shuffle,\n        shuffle_rng=shuffle_rng,\n        mesh=self._mesh)\n\n    if not is_train:\n        desc = f'{desc} (global_batch_size = {global_micro_batch_size})'\n    elif accumulate_grad_batches is None:\n        desc = \\\n            f'{desc} (global_micro_batch_size = {global_micro_batch_size})'\n    else:\n        desc = (f'{desc} ('\n                f'global_micro_batch_size = {global_micro_batch_size}, '\n                f'accumulate_grad_batches = {accumulate_grad_batches})')\n\n    return get_data_batches(\n        examples=examples,\n        batch_size=local_micro_batch_size,\n        collate_fn=collate_fn,\n        mesh=self._mesh,\n        desc=desc,\n        verbose=self._verbose)\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.get_opt_state_spec","title":"<code>get_opt_state_spec(params_shape_or_params, params_spec, optimizer)</code>","text":"<p>Get optimizer state specs</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def get_opt_state_spec(\n        self, params_shape_or_params, params_spec, optimizer):\n    \"\"\"Get optimizer state specs\"\"\"\n    return get_opt_state_spec(\n        params_shape_or_params=params_shape_or_params,\n        params_spec=params_spec,\n        optimizer=optimizer)\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.get_params_spec","title":"<code>get_params_spec(params_shape_or_params, params_sharding_rules)</code>","text":"<p>Generates parameter specs based on sharding rules.</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def get_params_spec(self, params_shape_or_params, params_sharding_rules):\n    \"\"\"Generates parameter specs based on sharding rules.\"\"\"\n    return get_params_spec(\n        params_shape_or_params=params_shape_or_params,\n        params_sharding_rules=params_sharding_rules)\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.get_sharding_rules","title":"<code>get_sharding_rules(params_shape_or_params)</code>","text":"<p>Get sharding rules based on the parameter shapes.</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def get_sharding_rules(self, params_shape_or_params):\n    \"\"\"Get sharding rules based on the parameter shapes.\"\"\"\n    if self._mesh is None:\n        return None\n    else:\n        sharding_rules = get_sharding_rules(\n            params_shape_or_params=params_shape_or_params,\n            n_model_shards=self._mesh.shape['mp'])\n        return sharding_rules\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.load_ckpt","title":"<code>load_ckpt(ckpt_dir, params_sharding_rules=None, optimizer=None, float_dtype=None, load_params=True, load_opt_state=True, update_rng=False)</code>","text":"<p>Loads a checkpoint from the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>ckpt_dir</code> <code>str</code> <p>Directory of the checkpoint.</p> required <code>params_sharding_rules</code> <code>list[tuple]</code> <p>Sharding rules for parameters.</p> <code>None</code> <code>optimizer</code> <code>optax optimizer</code> <p>Optimizer for loading opt_state.</p> <code>None</code> <code>float_dtype</code> <code>`jax.numpy.dtype`</code> <p>Dtype for floating point numbers.</p> <code>None</code> <code>load_params</code> <code>bool</code> <p>Whether to load the parameters.</p> <code>True</code> <code>load_opt_state</code> <code>bool</code> <p>Whether to load the optimizer state.</p> <code>True</code> <code>update_rng</code> <code>bool</code> <p>if updating the random state of the deployer.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple with the loaded checkpoint (in a dict with <code>\"params\"</code> and <code>\"opt_state\"</code>) and additional information (in a dict, usually including <code>\"steps\"</code>, <code>\"epoch_idx\"</code>, and <code>\"rng\"</code>).</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def load_ckpt(self,\n              ckpt_dir,\n              params_sharding_rules=None,\n              optimizer=None,\n              float_dtype=None,\n              load_params=True,\n              load_opt_state=True,\n              update_rng=False):\n    \"\"\"Loads a checkpoint from the specified directory.\n\n    Args:\n        ckpt_dir (str): Directory of the checkpoint.\n        params_sharding_rules (list[tuple]): Sharding rules for parameters.\n        optimizer (optax optimizer): Optimizer for loading opt_state.\n        float_dtype (`jax.numpy.dtype`): Dtype for floating point numbers.\n        load_params (bool): Whether to load the parameters.\n        load_opt_state (bool): Whether to load the optimizer state.\n        update_rng (bool): if updating the random state of the deployer.\n\n    Returns:\n        (tuple): A tuple with the loaded checkpoint (in a dict with\n            `\"params\"` and `\"opt_state\"`) and additional information (in a\n            dict, usually including `\"steps\"`, `\"epoch_idx\"`, and `\"rng\"`).\n    \"\"\"\n    ckpt_dir = os.path.abspath(ckpt_dir)\n    self.log_info(f'Loading ckpt from {ckpt_dir} ...')\n\n    params_shape = self.load_params_shape(ckpt_dir=ckpt_dir)\n\n    specs = {}\n    if self._mesh is not None:\n        if params_sharding_rules is None:\n            params_sharding_rules = self.get_sharding_rules(\n                params_shape_or_params=params_shape)\n\n        specs['params'] = self.get_params_spec(\n            params_shape_or_params=params_shape,\n            params_sharding_rules=params_sharding_rules)\n        if optimizer is not None:\n            specs['opt_state'] = self.get_opt_state_spec(\n                params_shape_or_params=params_shape,\n                params_spec=specs['params'],\n                optimizer=optimizer)\n\n    ckpt, info = load_ckpt(\n        ckpt_dir=ckpt_dir,\n        checkpointer=self._checkpointer,\n        params_shape_or_params=params_shape,\n        optimizer=optimizer,\n        float_dtype=float_dtype,\n        mesh=self._mesh,\n        specs=specs,\n        load_params=load_params,\n        load_opt_state=load_opt_state)\n\n    for key, value in info.items():\n        if not update_rng and key == 'rng':\n            continue\n        self.log_info(f'{ckpt_dir}::{key} = {value}')\n\n    if update_rng:\n        self._rng = info['rng']\n        self.log_info(f'rng updated to {self._rng} (by {ckpt_dir})')\n\n    return ckpt, info\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.load_last_ckpt","title":"<code>load_last_ckpt(optimizer=None, params_sharding_rules=None, float_dtype=None, load_params=True, load_opt_state=True, update_rng=True)</code>","text":"<p>Loads the last checkpoint from the work directory (self.workdir). See load_ckpt() for the explanation of arguments.</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def load_last_ckpt(self,\n                   optimizer=None,\n                   params_sharding_rules=None,\n                   float_dtype=None,\n                   load_params=True,\n                   load_opt_state=True,\n                   update_rng=True):\n    \"\"\"Loads the last checkpoint from the work directory (self.workdir).\n    See load_ckpt() for the explanation of arguments.\n    \"\"\"\n    try:\n        last_ckpt_name = open(\n            f'{self._workdir}/ckpts/last_ckpt.txt').read().strip()\n    except:\n        self.log_info(\n            f'{self._workdir}/ckpts/last_ckpt.txt not found. '\n            f'no ckpt loaded.')\n        return None, None\n\n    return self.load_ckpt(\n        ckpt_dir=f'{self._workdir}/ckpts/{last_ckpt_name}',\n        optimizer=optimizer,\n        float_dtype=float_dtype,\n        params_sharding_rules=params_sharding_rules,\n        load_params=load_params,\n        load_opt_state=load_opt_state,\n        update_rng=update_rng)\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.load_params_shape","title":"<code>load_params_shape(ckpt_dir)</code>","text":"<p>Loads the shape of the parameters from a checkpoint.</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def load_params_shape(self, ckpt_dir):\n    \"\"\"Loads the shape of the parameters from a checkpoint.\"\"\"\n    return load_params_shape(ckpt_dir=ckpt_dir)\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.log_info","title":"<code>log_info(info, title=None, step=None)</code>","text":"<p>Logs a messages</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def log_info(self, info, title=None, step=None):\n    \"\"\"Logs a messages\"\"\"\n    log_info(\n        info=info,\n        title=title,\n        logger=self._logger,\n        summary_writer=self._summary_writer,\n        step=step)\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.log_metrics","title":"<code>log_metrics(metrics, step)</code>","text":"<p>Logs metrics to TensorBoard and Weights and Biases (wandb).</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def log_metrics(self, metrics, step):\n    \"\"\"Logs metrics to TensorBoard and Weights and Biases (wandb).\"\"\"\n    if self._summary_writer is not None:\n        for metric_name, value in metrics.items():\n            self._summary_writer.scalar(metric_name, value, step=step)\n\n    if self._wandb_log_fn is not None:\n        self._wandb_log_fn(metrics, step)\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.run_model_step","title":"<code>run_model_step(step_fn, input_args)</code>","text":"<p>Executes a model step function with the provided inputs.</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def run_model_step(self, step_fn, input_args):\n    \"\"\"Executes a model step function with the provided inputs.\"\"\"\n    if self._mesh is None:\n        return step_fn(*input_args)\n    else:\n        with self._mesh:\n            return step_fn(*input_args)\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.save_ckpt","title":"<code>save_ckpt(ckpt_dir, params, opt_state=None, float_dtype=None, **kwargs)</code>","text":"<p>Saves a checkpoint to the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>ckpt_dir</code> <code>str</code> <p>Directory to save the checkpoint.</p> required <code>params</code> <code>dict</code> <p>Model parameters.</p> required <code>opt_state</code> <code>dict</code> <p>Optimizer state.</p> <code>None</code> <code>float_dtype</code> <code>`jax.numpy.dtype`</code> <p>Dtype for floating point numbers.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional information to be saved into info.json, e.g., current training step, epoch index, etc.</p> <code>{}</code> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def save_ckpt(\n        self, ckpt_dir, params, opt_state=None, float_dtype=None, **kwargs):\n    \"\"\"Saves a checkpoint to the specified directory.\n\n    Args:\n        ckpt_dir (str): Directory to save the checkpoint.\n        params (dict): Model parameters.\n        opt_state (dict): Optimizer state.\n        float_dtype (`jax.numpy.dtype`): Dtype for floating point numbers.\n        **kwargs (dict): Additional information to be saved into\n            info.json, e.g., current training step, epoch index, etc.\n    \"\"\"\n    ckpt_dir = os.path.abspath(ckpt_dir)\n    self.log_info(f'Saving ckpt to {ckpt_dir} ...')\n    save_ckpt(\n        ckpt_dir=ckpt_dir,\n        checkpointer=self._checkpointer,\n        params=params,\n        opt_state=opt_state,\n        float_dtype=float_dtype,\n        rng=self._rng,\n        **kwargs)\n    self.log_info(f'Ckpt saved into {ckpt_dir}')\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.save_outputs","title":"<code>save_outputs(outputs, desc, step)</code>","text":"<p>Saves model outputs to workdir.</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def save_outputs(self, outputs, desc, step):\n    \"\"\"Saves model outputs to workdir.\"\"\"\n    if self._workdir is not None and jax.process_index() == 0:\n        save_outputs(\n            workdir=self._workdir,\n            outputs=outputs,\n            desc=desc,\n            step=step,\n            logger=self._logger,\n            summary_writer=self._summary_writer)\n</code></pre>"},{"location":"docs/deployer/#redco.deployers.deployer.Deployer.shard_params","title":"<code>shard_params(params, params_spec, desc='params')</code>","text":"<p>Distributes parameters to all devices based on the provided specs.</p> Source code in <code>redco/deployers/deployer.py</code> <pre><code>def shard_params(self, params, params_spec, desc='params'):\n    \"\"\"Distributes parameters to all devices based on the provided specs.\"\"\"\n    self.log_info(info=f'Sharding {desc} ...')\n    return shard_params(\n        mesh=self._mesh, params=params, params_spec=params_spec)\n</code></pre>"},{"location":"docs/predictor/","title":"Predictor","text":""},{"location":"docs/predictor/#redco.predictors.predictor.Predictor","title":"<code>Predictor</code>","text":"<p>Predictor class managing distributed inference process.</p> <p>Attributes:</p> Name Type Description <code>mesh</code> <code>jax Mesh</code> <p>Mesh used for distributed inference.</p> Source code in <code>redco/predictors/predictor.py</code> <pre><code>class Predictor:\n    \"\"\"Predictor class managing distributed inference process.\n\n    Attributes:\n        mesh (jax Mesh): Mesh used for distributed inference.\n    \"\"\"\n    def __init__(self,\n                 deployer,\n                 collate_fn,\n                 pred_fn,\n                 output_fn=None,\n                 params_sharding_rules=None):\n        \"\"\"Initializes a Predictor instance.\n\n        Args:\n            deployer (Deployer): A deployer for low-level operations.\n            collate_fn (Callable): A function making model inputs from raw data,\n                e.g., tokenizing sentences into input_ids.\n            pred_fn (Callable): A function producing model outputs from inputs,\n                e.g., running beam search with a language model.\n            output_fn (Callable): A function post-processing model outputs,\n                e.g., decoding generated ids to text.\n            params_sharding_rules (list[tuple]): Rules for sharding parameters.\n        \"\"\"\n        self._deployer = deployer\n        self._collate_fn = partial(collate_fn_wrapper, collate_fn=collate_fn)\n        self._params_sharding_rules = params_sharding_rules\n        self._pred_fn = partial(pred_fn_wrapper, pred_fn=pred_fn)\n        self._p_pred_step = None\n\n        if output_fn is None:\n            self._output_fn = default_output_fn\n        else:\n            self._output_fn = output_fn\n\n    def setup_running_step(self, dummy_batch, params_shape_or_params):\n        \"\"\"Sets up the prediction step function for distributed inference.\n\n        Args:\n            dummy_batch (PyTree): A dummy batch used to determine data shapes.\n            params_shape_or_params (dict): The shape of params or actual params.\n        \"\"\"\n        pred_step_fn = partial(pred_step, pred_fn=self._pred_fn, mesh=self.mesh)\n\n        if self.mesh is None:\n            self._p_pred_step = jax.pmap(pred_step_fn, axis_name='dp')\n        else:\n            data_spec = jax.tree.map(lambda x: P('dp'), dummy_batch)\n            params_spec = self._deployer.get_params_spec(\n                params_shape_or_params=params_shape_or_params,\n                params_sharding_rules=self._params_sharding_rules)\n            self._p_pred_step = pjit(\n                pred_step_fn,\n                in_shardings=(None, params_spec, data_spec),\n                out_shardings=None)\n\n    def predict(self,\n                examples,\n                per_device_batch_size,\n                params,\n                params_replicated=False,\n                params_sharded=False,\n                desc=None):\n        \"\"\"Runs distributed prediction on a list of examples.\n\n        Args:\n            examples (list): Input examples for prediction.\n            per_device_batch_size (int): Batch size per device.\n            params (dict): Model parameters in a dict/FrozenDict.\n            params_replicated (bool): if the params are already replicated.\n            params_sharded (bool): if the parameters are already sharded.\n            desc (str): Description to show in the progress bar.\n\n        Returns:\n            (list): A list of predictions corresponding to the input examples.\n        \"\"\"\n        raw_n_inputs = len(examples)\n        _, global_micro_batch_size = \\\n            self._deployer.get_local_global_micro_batch_size(\n                per_device_batch_size=per_device_batch_size)\n        examples = examples + [examples[0]] * (global_micro_batch_size - 1)\n        examples = add_idxes(examples=examples)\n\n        data_batches = self._deployer.get_model_input_batches(\n            examples=examples,\n            per_device_batch_size=per_device_batch_size,\n            collate_fn=self._collate_fn,\n            shuffle=False,\n            shuffle_rng=None,\n            desc=f'Predicting ({desc})' if desc is not None else 'Predicting')\n\n        params = freeze(params)\n        if (self.mesh is None) and (not params_replicated):\n            params = replicate(params)\n        if (self.mesh is not None) and (not params_sharded):\n            params_spec = self._deployer.get_params_spec(\n                params_shape_or_params=params,\n                params_sharding_rules=self._params_sharding_rules)\n            params = self._deployer.shard_params(\n                params=params, params_spec=params_spec)\n\n        preds = []\n        for batch in data_batches:\n            if self._p_pred_step is None:\n                self.setup_running_step(\n                    dummy_batch=batch, params_shape_or_params=params)\n\n            rng = self._deployer.gen_model_step_rng()\n            batch_preds_with_idxes = self._deployer.run_model_step(\n                step_fn=self._p_pred_step, input_args=(rng, params, batch))\n            batch_preds = process_batch_preds(\n                batch_preds_with_idxes=batch_preds_with_idxes, mesh=self.mesh)\n            batch_preds = self._output_fn(batch_preds)\n\n            assert isinstance(batch_preds, list) and \\\n                   len(batch_preds) == global_micro_batch_size\n            preds.extend(batch_preds)\n\n        return preds[:raw_n_inputs]\n\n    @property\n    def mesh(self):\n        \"\"\"Returns the mesh used for distributed inference.\"\"\"\n        return self._deployer.mesh\n</code></pre>"},{"location":"docs/predictor/#redco.predictors.predictor.Predictor.mesh","title":"<code>mesh</code>  <code>property</code>","text":"<p>Returns the mesh used for distributed inference.</p>"},{"location":"docs/predictor/#redco.predictors.predictor.Predictor.__init__","title":"<code>__init__(deployer, collate_fn, pred_fn, output_fn=None, params_sharding_rules=None)</code>","text":"<p>Initializes a Predictor instance.</p> <p>Parameters:</p> Name Type Description Default <code>deployer</code> <code>Deployer</code> <p>A deployer for low-level operations.</p> required <code>collate_fn</code> <code>Callable</code> <p>A function making model inputs from raw data, e.g., tokenizing sentences into input_ids.</p> required <code>pred_fn</code> <code>Callable</code> <p>A function producing model outputs from inputs, e.g., running beam search with a language model.</p> required <code>output_fn</code> <code>Callable</code> <p>A function post-processing model outputs, e.g., decoding generated ids to text.</p> <code>None</code> <code>params_sharding_rules</code> <code>list[tuple]</code> <p>Rules for sharding parameters.</p> <code>None</code> Source code in <code>redco/predictors/predictor.py</code> <pre><code>def __init__(self,\n             deployer,\n             collate_fn,\n             pred_fn,\n             output_fn=None,\n             params_sharding_rules=None):\n    \"\"\"Initializes a Predictor instance.\n\n    Args:\n        deployer (Deployer): A deployer for low-level operations.\n        collate_fn (Callable): A function making model inputs from raw data,\n            e.g., tokenizing sentences into input_ids.\n        pred_fn (Callable): A function producing model outputs from inputs,\n            e.g., running beam search with a language model.\n        output_fn (Callable): A function post-processing model outputs,\n            e.g., decoding generated ids to text.\n        params_sharding_rules (list[tuple]): Rules for sharding parameters.\n    \"\"\"\n    self._deployer = deployer\n    self._collate_fn = partial(collate_fn_wrapper, collate_fn=collate_fn)\n    self._params_sharding_rules = params_sharding_rules\n    self._pred_fn = partial(pred_fn_wrapper, pred_fn=pred_fn)\n    self._p_pred_step = None\n\n    if output_fn is None:\n        self._output_fn = default_output_fn\n    else:\n        self._output_fn = output_fn\n</code></pre>"},{"location":"docs/predictor/#redco.predictors.predictor.Predictor.predict","title":"<code>predict(examples, per_device_batch_size, params, params_replicated=False, params_sharded=False, desc=None)</code>","text":"<p>Runs distributed prediction on a list of examples.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>list</code> <p>Input examples for prediction.</p> required <code>per_device_batch_size</code> <code>int</code> <p>Batch size per device.</p> required <code>params</code> <code>dict</code> <p>Model parameters in a dict/FrozenDict.</p> required <code>params_replicated</code> <code>bool</code> <p>if the params are already replicated.</p> <code>False</code> <code>params_sharded</code> <code>bool</code> <p>if the parameters are already sharded.</p> <code>False</code> <code>desc</code> <code>str</code> <p>Description to show in the progress bar.</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of predictions corresponding to the input examples.</p> Source code in <code>redco/predictors/predictor.py</code> <pre><code>def predict(self,\n            examples,\n            per_device_batch_size,\n            params,\n            params_replicated=False,\n            params_sharded=False,\n            desc=None):\n    \"\"\"Runs distributed prediction on a list of examples.\n\n    Args:\n        examples (list): Input examples for prediction.\n        per_device_batch_size (int): Batch size per device.\n        params (dict): Model parameters in a dict/FrozenDict.\n        params_replicated (bool): if the params are already replicated.\n        params_sharded (bool): if the parameters are already sharded.\n        desc (str): Description to show in the progress bar.\n\n    Returns:\n        (list): A list of predictions corresponding to the input examples.\n    \"\"\"\n    raw_n_inputs = len(examples)\n    _, global_micro_batch_size = \\\n        self._deployer.get_local_global_micro_batch_size(\n            per_device_batch_size=per_device_batch_size)\n    examples = examples + [examples[0]] * (global_micro_batch_size - 1)\n    examples = add_idxes(examples=examples)\n\n    data_batches = self._deployer.get_model_input_batches(\n        examples=examples,\n        per_device_batch_size=per_device_batch_size,\n        collate_fn=self._collate_fn,\n        shuffle=False,\n        shuffle_rng=None,\n        desc=f'Predicting ({desc})' if desc is not None else 'Predicting')\n\n    params = freeze(params)\n    if (self.mesh is None) and (not params_replicated):\n        params = replicate(params)\n    if (self.mesh is not None) and (not params_sharded):\n        params_spec = self._deployer.get_params_spec(\n            params_shape_or_params=params,\n            params_sharding_rules=self._params_sharding_rules)\n        params = self._deployer.shard_params(\n            params=params, params_spec=params_spec)\n\n    preds = []\n    for batch in data_batches:\n        if self._p_pred_step is None:\n            self.setup_running_step(\n                dummy_batch=batch, params_shape_or_params=params)\n\n        rng = self._deployer.gen_model_step_rng()\n        batch_preds_with_idxes = self._deployer.run_model_step(\n            step_fn=self._p_pred_step, input_args=(rng, params, batch))\n        batch_preds = process_batch_preds(\n            batch_preds_with_idxes=batch_preds_with_idxes, mesh=self.mesh)\n        batch_preds = self._output_fn(batch_preds)\n\n        assert isinstance(batch_preds, list) and \\\n               len(batch_preds) == global_micro_batch_size\n        preds.extend(batch_preds)\n\n    return preds[:raw_n_inputs]\n</code></pre>"},{"location":"docs/predictor/#redco.predictors.predictor.Predictor.setup_running_step","title":"<code>setup_running_step(dummy_batch, params_shape_or_params)</code>","text":"<p>Sets up the prediction step function for distributed inference.</p> <p>Parameters:</p> Name Type Description Default <code>dummy_batch</code> <code>PyTree</code> <p>A dummy batch used to determine data shapes.</p> required <code>params_shape_or_params</code> <code>dict</code> <p>The shape of params or actual params.</p> required Source code in <code>redco/predictors/predictor.py</code> <pre><code>def setup_running_step(self, dummy_batch, params_shape_or_params):\n    \"\"\"Sets up the prediction step function for distributed inference.\n\n    Args:\n        dummy_batch (PyTree): A dummy batch used to determine data shapes.\n        params_shape_or_params (dict): The shape of params or actual params.\n    \"\"\"\n    pred_step_fn = partial(pred_step, pred_fn=self._pred_fn, mesh=self.mesh)\n\n    if self.mesh is None:\n        self._p_pred_step = jax.pmap(pred_step_fn, axis_name='dp')\n    else:\n        data_spec = jax.tree.map(lambda x: P('dp'), dummy_batch)\n        params_spec = self._deployer.get_params_spec(\n            params_shape_or_params=params_shape_or_params,\n            params_sharding_rules=self._params_sharding_rules)\n        self._p_pred_step = pjit(\n            pred_step_fn,\n            in_shardings=(None, params_spec, data_spec),\n            out_shardings=None)\n</code></pre>"},{"location":"docs/trainer/","title":"Trainer","text":""},{"location":"docs/trainer/#redco.trainers.trainer.Trainer","title":"<code>Trainer</code>","text":"<p>Trainer class managing distributed training process.</p> <p>Attributes:</p> Name Type Description <code>step</code> <code>int</code> <p>Current training step.</p> <code>workdir</code> <code>str</code> <p>Working directory for saving checkpoints and logs.</p> <code>mesh</code> <code>jax Mesh</code> <p>Mesh used for distributed training.</p> <code>state</code> <code>flax TrainState</code> <p>Current training state.</p> Source code in <code>redco/trainers/trainer.py</code> <pre><code>class Trainer:\n    \"\"\"Trainer class managing distributed training process.\n\n    Attributes:\n        step (int): Current training step.\n        workdir (str): Working directory for saving checkpoints and logs.\n        mesh (jax Mesh): Mesh used for distributed training.\n        state (flax TrainState): Current training state.\n    \"\"\"\n    def __init__(self,\n                 deployer,\n                 collate_fn,\n                 apply_fn,\n                 loss_fn,\n                 params,\n                 optimizer,\n                 opt_state=None,\n                 compute_dtype=jnp.float32,\n                 last_ckpt_info=None,\n                 lr_schedule_fn=None,\n                 accumulate_grad_batches=None,\n                 params_sharding_rules=None,\n                 train_step_fn=None):\n        \"\"\"Initializes the Trainer with initial parameters, etc.\n\n        Args:\n            deployer (Deployer): A deployer supporting low-level operations.\n            collate_fn (Callable): The function converting a data batch to model\n                inputs, e.g., tokenizing sentences into input_ids.\n            apply_fn (Callable): The function to apply the model, such as\n                model.apply for Flax modules, or model itself for HuggingFace\n                models. It would be set as state.apply_fn, and used in loss_fn.\n            loss_fn (Callable): The loss function converting model inputs to a\n                scalar loss, e.g., computing cross-entropy loss from input_ids.\n            params (dict): Initial model parameters.\n            optimizer (optax optimizer): The optimizer used for training.\n            opt_state (dict): optimizer state.\n            compute_dtype (dtype): Computation dtype, e.g., `jnp.bfloat16`,\n                independent of param dtypes. (for mixed-precision training)\n            last_ckpt_info (dict): the beginning step and epoch.\n            lr_schedule_fn (Callable): The learning rate schedule\n                function converting step to learning rate.\n            accumulate_grad_batches (int): Gradient accumulation step.\n            params_sharding_rules (list): Sharding rules.\n            train_step_fn (Callable): For fully customizing every training step,\n                e.g., per-sample gradient noising for data-private training.\n        \"\"\"\n        self._deployer = deployer\n        self._collate_fn = collate_fn\n        self._apply_fn = apply_fn\n        self._loss_fn = loss_fn\n        self._optimizer = optimizer\n        self._compute_dtype = compute_dtype\n        self._lr_schedule_fn = lr_schedule_fn\n        self._accumulate_grad_batches = accumulate_grad_batches\n        self._params_sharding_rules = params_sharding_rules\n        self._train_step_fn = train_step_fn\n\n        self._state = None\n        self._state_spec = None\n        self._p_train_step = None\n        self._p_eval_step = None\n\n        self._init_step = 0\n        self._init_epoch_idx = 0\n        if last_ckpt_info is not None:\n            self._init_step = last_ckpt_info.get('step', 0)\n            self._init_epoch_idx = last_ckpt_info.get('epoch_idx', -1) + 1\n\n        n_params = sum([param.size for param in jax.tree.leaves(params)])\n        self._deployer.log_info(f'{n_params:,}', title='Parameters')\n\n        self.set_train_state(\n            apply_fn=self._apply_fn,\n            params=params,\n            optimizer=self._optimizer,\n            step=self._init_step,\n            opt_state=opt_state)\n\n    def set_train_state(\n            self, apply_fn, params, optimizer, step, opt_state=None):\n        \"\"\"Sets/Resets the training state with given parameters and optimizer.\n\n        Args:\n            apply_fn (Callable): The function to apply the model.\n            params (dict): Model parameters.\n            optimizer (dict): The optimizer used for training.\n            step (int): The training step.\n            opt_state (dict): The state of the optimizer.\n        \"\"\"\n        self._deployer.log_info('Setting train_state ...')\n        params = freeze(params)\n\n        if self.mesh is None:\n            params = jax.device_put(params, jax.local_devices()[0])\n            if opt_state is None:\n                self._deployer.log_info('Initializing opt_state ...')\n                opt_state = optimizer.init(params)\n            else:\n                opt_state = jax.device_put(opt_state, jax.local_devices()[0])\n\n            self._state = train_state.TrainState(\n                step=step,\n                apply_fn=apply_fn,\n                params=params,\n                tx=optimizer,\n                opt_state=opt_state)\n            self._state = replicate(self._state)\n        else:\n            params_spec = self._deployer.get_params_spec(\n                params_shape_or_params=params,\n                params_sharding_rules=self._params_sharding_rules)\n            params = self._deployer.shard_params(\n                params=params, params_spec=params_spec)\n\n            if opt_state is None:\n                self._deployer.log_info('Initializing opt_state ...')\n                opt_state = optimizer.init(params)\n\n            opt_state_spec = self._deployer.get_opt_state_spec(\n                params_shape_or_params=params,\n                params_spec=params_spec,\n                optimizer=optimizer)\n            opt_state = self._deployer.shard_params(\n                params=opt_state,\n                params_spec=opt_state_spec,\n                desc='opt_state')\n\n            self._state = train_state.TrainState(\n                apply_fn=apply_fn,\n                params=params,\n                tx=optimizer,\n                opt_state=opt_state,\n                step=step)\n\n            self._state_spec = train_state.TrainState(\n                apply_fn=apply_fn,\n                params=params_spec,\n                tx=optimizer,\n                opt_state=opt_state_spec,\n                step=None)\n\n    def setup_running_step(self, dummy_batch):\n        \"\"\"Sets up the running step functions for training and evaluation.\n\n        Args:\n            dummy_batch (PyTree): A dummy batch of data.\n        \"\"\"\n        train_step_fn = partial(\n            self._train_step_fn or default_train_step,\n            loss_fn=self._loss_fn,\n            lr_schedule_fn=self._lr_schedule_fn,\n            mesh=self.mesh,\n            compute_dtype=self._compute_dtype)\n        eval_step_fn = partial(\n            eval_step,\n            loss_fn=self._loss_fn,\n            mesh=self.mesh,\n            compute_dtype=self._compute_dtype)\n\n        if self.mesh is None:\n            self._p_train_step = jax.pmap(train_step_fn, axis_name='dp')\n            self._p_eval_step = jax.pmap(eval_step_fn, axis_name='dp')\n        else:\n            data_spec = jax.tree.map(lambda x: P('dp'), dummy_batch)\n            self._p_train_step = pjit(\n                train_step_fn,\n                in_shardings=(None, self._state_spec, data_spec),\n                out_shardings=(self._state_spec, None),\n                donate_argnums=(1, ))\n            self._p_eval_step = pjit(\n                eval_step_fn,\n                in_shardings=(None, self._state_spec, data_spec),\n                out_shardings=None)\n\n    def train(self, examples, per_device_batch_size, desc=None):\n        \"\"\"Trains the model on the provided examples.\n\n        Args:\n            examples (list): Training examples in python list.\n            per_device_batch_size (int): The batch size per device.\n            desc (str): Description in the progress bar.\n        \"\"\"\n        data_batches = self._deployer.get_model_input_batches(\n            examples=examples,\n            per_device_batch_size=per_device_batch_size,\n            collate_fn=self._collate_fn,\n            shuffle=True,\n            shuffle_rng=self._deployer.gen_rng(),\n            desc=f'Training ({desc})' if desc is not None else 'Training',\n            is_train=True,\n            accumulate_grad_batches=self._accumulate_grad_batches)\n\n        for batch in data_batches:\n            if self._p_train_step is None:\n                self.setup_running_step(dummy_batch=batch)\n\n            rng = self._deployer.gen_model_step_rng()\n            self._state, metrics = self._deployer.run_model_step(\n                step_fn=self._p_train_step,\n                input_args=(rng, self._state, batch))\n\n            if self.mesh is None:\n                metrics = unreplicate(metrics)\n            data_batches.set_postfix(**metrics)\n            self._deployer.log_metrics(metrics=metrics, step=self.step)\n\n    def eval_loss(self, examples, per_device_batch_size, desc=None):\n        \"\"\"Evaluates the loss on the provided examples.\n\n        Args:\n            examples (list): Evaluation examples in list.\n            per_device_batch_size (int): The batch size per device.\n            desc (str): Description in the progress bar.\n\n        Returns:\n            (float): The average loss over the evaluation examples.\n        \"\"\"\n        data_batches = self._deployer.get_model_input_batches(\n            examples=examples,\n            per_device_batch_size=per_device_batch_size,\n            collate_fn=self._collate_fn,\n            shuffle=False,\n            shuffle_rng=None,\n            desc=f'Evaluating ({desc})' if desc is not None else 'Evaluating')\n\n        losses = []\n        for batch in data_batches:\n            if self._p_eval_step is None:\n                self.setup_running_step(dummy_batch=batch)\n\n            rng = self._deployer.gen_model_step_rng()\n            metrics = self._deployer.run_model_step(\n                step_fn=self._p_eval_step, input_args=(rng, self._state, batch))\n            if self.mesh is None:\n                metrics = unreplicate(metrics)\n\n            losses.append(metrics['loss'].item())\n            data_batches.set_postfix(**metrics)\n\n        return np.mean(losses).item()\n\n    def fit(self,\n            train_examples,\n            per_device_batch_size,\n            n_epochs,\n            eval_examples=None,\n            eval_per_device_batch_size=None,\n            eval_loss=True,\n            eval_predictor=None,\n            eval_metric_fn=None,\n            eval_sanity_check=True,\n            save_every_ckpt=False,\n            save_last_ckpt=False,\n            save_argmin_ckpt_by_metrics=None,\n            save_argmax_ckpt_by_metrics=None,\n            save_opt_states=True,\n            save_float_dtype=None):\n        \"\"\"Fits the model on the training data for a given number of epochs,\n        optionally evaluating and saving checkpoints.\n\n        Args:\n            train_examples (list or Callable): Training examples, can be a\n                list or a function of epoch_idx (for assigning different\n                examples in separate epochs/chunks),\n                e.g., `train_examples=lambda epoch_idx: load_data(chunk_idx)`\n            per_device_batch_size (int): The batch size per device.\n            n_epochs (int): Number of epochs to train.\n            eval_examples (list): Examples for evaluation and prediction.\n            eval_per_device_batch_size (int): Batch size for evaluation\n            eval_loss (bool): Whether to evaluate loss.\n            eval_predictor (Predictor): Predictor working on `eval_examples`.\n            eval_metric_fn (Callable): Metric function for prediction.\n            eval_sanity_check (bool): if to run a sanity check for\n                evaluation &amp; predict functions before training.\n            save_every_ckpt (bool): if to save a ckpt after every epoch.\n            save_last_ckpt (bool): Whether to save the last checkpoint.\n            save_argmin_ckpt_by_metrics (list[str]): Metrics to save checkpoints\n                based on minimum values.\n            save_argmax_ckpt_by_metrics (list[str]): Metrics to save checkpoints\n                based on maximum values.\n            save_opt_states (bool): of to save optimizer states in ckpts.\n            save_float_dtype (bool): The data type for saving checkpoints.\n        \"\"\"\n        if eval_per_device_batch_size is None:\n            eval_per_device_batch_size = per_device_batch_size\n\n        if save_argmax_ckpt_by_metrics is None:\n            save_argmax_ckpt_by_metrics = []\n        if save_argmin_ckpt_by_metrics is None:\n            save_argmin_ckpt_by_metrics = []\n        min_metrics, max_metrics = {}, {}\n\n        if os.path.exists(f'{self.workdir}/min_metrics.json'):\n            min_metrics = json.load(open(\n                f'{self.workdir}/min_metrics.json'))\n            self._deployer.log_info(\n                json.dumps(min_metrics, indent=4), title='Detected min_metrics')\n\n        if os.path.exists(f'{self.workdir}/max_metrics.json'):\n            max_metrics = json.load(open(\n                f'{self.workdir}/max_metrics.json'))\n            self._deployer.log_info(\n                json.dumps(max_metrics, indent=4), title='Detected max_metrics')\n\n        if eval_sanity_check and eval_examples is not None:\n            rng_backup = self._deployer._rng\n            _, eval_global_micro_batch_size = \\\n                self._deployer.get_local_global_micro_batch_size(\n                    per_device_batch_size=eval_per_device_batch_size)\n\n            if eval_loss:\n                self.eval_loss(\n                    examples=eval_examples[:eval_global_micro_batch_size],\n                    per_device_batch_size=eval_per_device_batch_size,\n                    desc=f'Sanity check')\n                self._deployer.log_info(\n                    'Sanity check (for evaluation loss) passed.')\n\n            if eval_predictor is not None:\n                preds = eval_predictor.predict(\n                    examples=eval_examples[:eval_global_micro_batch_size],\n                    params=self._state.params,\n                    params_replicated=(self.mesh is None),\n                    params_sharded=(self.mesh is not None),\n                    per_device_batch_size=eval_per_device_batch_size,\n                    desc=f'Sanity check')\n                self._deployer.log_info(\n                    'Sanity check (for prediction) passed.')\n\n                if eval_metric_fn is not None:\n                    json.dumps(eval_metric_fn(\n                        examples=eval_examples[:eval_global_micro_batch_size],\n                        preds=preds))\n                    self._deployer.log_info(\n                        'Sanity check (for evaluation metrics) passed.')\n\n            self._deployer._rng = rng_backup\n\n        for epoch_idx in range(self._init_epoch_idx, n_epochs):\n            if isinstance(train_examples, list):\n                epoch_train_examples = train_examples\n            else:\n                epoch_train_examples = train_examples(epoch_idx=epoch_idx)\n\n            self.train(\n                examples=epoch_train_examples,\n                per_device_batch_size=per_device_batch_size,\n                desc=f'epoch {epoch_idx} / {n_epochs}')\n\n            save_ckpt_kwargs = {\n                'epoch_idx': epoch_idx,\n                'save_opt_state': save_opt_states,\n                'float_dtype': save_float_dtype\n            }\n\n            if eval_examples is None:\n                self._deployer.log_info(\n                    'No evaluation cuz \\'eval_examples\\' is None.')\n            else:\n                eval_metrics = {}\n\n                if eval_loss:\n                    loss = self.eval_loss(\n                        examples=eval_examples,\n                        per_device_batch_size=eval_per_device_batch_size,\n                        desc=f'epoch {epoch_idx} / {n_epochs}')\n                    eval_metrics['loss'] = loss\n\n                if eval_predictor is not None:\n                    preds = eval_predictor.predict(\n                        examples=eval_examples,\n                        params=self._state.params,\n                        params_replicated=(self.mesh is None),\n                        params_sharded=(self.mesh is not None),\n                        per_device_batch_size=eval_per_device_batch_size,\n                        desc=f'epoch {epoch_idx} / {n_epochs}')\n\n                    if eval_metric_fn is not None:\n                        eval_metrics.update(eval_metric_fn(\n                            examples=eval_examples, preds=preds))\n\n                    eval_outputs = [\n                        {'example': example, 'pred': pred}\n                        for example, pred in zip(eval_examples, preds)]\n\n                    self._deployer.save_outputs(\n                        outputs=eval_outputs,\n                        desc=f'epoch{epoch_idx}',\n                        step=self.step)\n\n                self._deployer.log_info(\n                    info=json.dumps(eval_metrics, indent=4),\n                    title=f'Eval results',\n                    step=self.step)\n                self._deployer.log_metrics(metrics={\n                    f'eval_{key}': value\n                    for key, value in eval_metrics.items()\n                }, step=self.step)\n\n                if self.workdir is not None:\n                    result_filepath = \\\n                        f'{self.workdir}/eval_results_epoch{epoch_idx}.json'\n                    json.dump(\n                        eval_metrics, open(result_filepath, 'w'), indent=4)\n                    self._deployer.log_info(\n                        f'eval_results saved into {result_filepath}.')\n\n                for key in save_argmin_ckpt_by_metrics:\n                    assert self.workdir is not None\n                    if eval_metrics[key] &lt; min_metrics.get(key, float('inf')):\n                        min_metrics[key] = eval_metrics[key]\n\n                        if jax.process_index() == 0:\n                            self._deployer.log_info(\n                                f'minimal {key} updated to {min_metrics[key]}')\n                            json.dump(min_metrics, open(\n                                f'{self.workdir}/min_metrics.json', 'w'))\n\n                        self.save_ckpt(\n                            ckpt_name=f'min_{key}', **save_ckpt_kwargs)\n\n                for key in save_argmax_ckpt_by_metrics:\n                    assert self.workdir is not None\n                    if eval_metrics[key] &gt; max_metrics.get(key, float('-inf')):\n                        max_metrics[key] = eval_metrics[key]\n\n                        if jax.process_index() == 0:\n                            self._deployer.log_info(\n                                f'maximal {key} updated to {max_metrics[key]}')\n                            json.dump(max_metrics, open(\n                                f'{self.workdir}/max_metrics.json', 'w'))\n\n                        self.save_ckpt(\n                            ckpt_name=f'max_{key}', **save_ckpt_kwargs)\n\n            if save_every_ckpt:\n                self.save_ckpt(\n                    ckpt_name=f'epoch_{epoch_idx}', **save_ckpt_kwargs)\n            elif save_last_ckpt:\n                self.save_ckpt(ckpt_name='last', **save_ckpt_kwargs)\n\n    def save_ckpt(self, epoch_idx, ckpt_name, save_opt_state, float_dtype):\n        \"\"\"Saves a checkpoint into `{self.workdir}/ckpts`.\n\n        Args:\n            epoch_idx (int): The current epoch index.\n            ckpt_name (str): The name of the checkpoint.\n            save_opt_state (bool): Whether to save the optimizer state.\n            float_dtype (`jax.numpy.dtype`): Data type for saving float params.\n        \"\"\"\n        if self.mesh is None:\n            params = jax.tree.map(\n                fully_replicated_host_local_array_to_global_array,\n                self._state.params)\n        else:\n            params = self._state.params\n\n        opt_state = None\n        if save_opt_state:\n            if self.mesh is None:\n                opt_state = jax.tree.map(\n                    fully_replicated_host_local_array_to_global_array,\n                    self._state.opt_state)\n            else:\n                opt_state = self._state.opt_state\n\n        ckpt_dir = f'{self.workdir}/ckpts/{ckpt_name}'\n        self._deployer.save_ckpt(\n            ckpt_dir=ckpt_dir,\n            params=params,\n            opt_state=opt_state,\n            float_dtype=float_dtype,\n            step=self.step,\n            epoch_idx=epoch_idx)\n\n        if jax.process_index() == 0:\n            open(f'{self.workdir}/ckpts/last_ckpt.txt', 'w').write(ckpt_name)\n            self._deployer.log_info(f'last ckpt updated -- {ckpt_dir}')\n\n    @property\n    def step(self):\n        \"\"\"Returns the current training step.\"\"\"\n        if self.mesh is None:\n            return unreplicate(self._state.step).item()\n        else:\n            return self._state.step.item()\n\n    @property\n    def workdir(self):\n        \"\"\"Returns the working directory for saving checkpoints and logs.\"\"\"\n        return self._deployer.workdir\n\n    @property\n    def mesh(self):\n        \"\"\"Returns the mesh used for distributed training.\"\"\"\n        return self._deployer.mesh\n\n    @property\n    def state(self):\n        \"\"\"Returns the current training state.\"\"\"\n        return self._state\n</code></pre>"},{"location":"docs/trainer/#redco.trainers.trainer.Trainer.mesh","title":"<code>mesh</code>  <code>property</code>","text":"<p>Returns the mesh used for distributed training.</p>"},{"location":"docs/trainer/#redco.trainers.trainer.Trainer.state","title":"<code>state</code>  <code>property</code>","text":"<p>Returns the current training state.</p>"},{"location":"docs/trainer/#redco.trainers.trainer.Trainer.step","title":"<code>step</code>  <code>property</code>","text":"<p>Returns the current training step.</p>"},{"location":"docs/trainer/#redco.trainers.trainer.Trainer.workdir","title":"<code>workdir</code>  <code>property</code>","text":"<p>Returns the working directory for saving checkpoints and logs.</p>"},{"location":"docs/trainer/#redco.trainers.trainer.Trainer.__init__","title":"<code>__init__(deployer, collate_fn, apply_fn, loss_fn, params, optimizer, opt_state=None, compute_dtype=jnp.float32, last_ckpt_info=None, lr_schedule_fn=None, accumulate_grad_batches=None, params_sharding_rules=None, train_step_fn=None)</code>","text":"<p>Initializes the Trainer with initial parameters, etc.</p> <p>Parameters:</p> Name Type Description Default <code>deployer</code> <code>Deployer</code> <p>A deployer supporting low-level operations.</p> required <code>collate_fn</code> <code>Callable</code> <p>The function converting a data batch to model inputs, e.g., tokenizing sentences into input_ids.</p> required <code>apply_fn</code> <code>Callable</code> <p>The function to apply the model, such as model.apply for Flax modules, or model itself for HuggingFace models. It would be set as state.apply_fn, and used in loss_fn.</p> required <code>loss_fn</code> <code>Callable</code> <p>The loss function converting model inputs to a scalar loss, e.g., computing cross-entropy loss from input_ids.</p> required <code>params</code> <code>dict</code> <p>Initial model parameters.</p> required <code>optimizer</code> <code>optax optimizer</code> <p>The optimizer used for training.</p> required <code>opt_state</code> <code>dict</code> <p>optimizer state.</p> <code>None</code> <code>compute_dtype</code> <code>dtype</code> <p>Computation dtype, e.g., <code>jnp.bfloat16</code>, independent of param dtypes. (for mixed-precision training)</p> <code>float32</code> <code>last_ckpt_info</code> <code>dict</code> <p>the beginning step and epoch.</p> <code>None</code> <code>lr_schedule_fn</code> <code>Callable</code> <p>The learning rate schedule function converting step to learning rate.</p> <code>None</code> <code>accumulate_grad_batches</code> <code>int</code> <p>Gradient accumulation step.</p> <code>None</code> <code>params_sharding_rules</code> <code>list</code> <p>Sharding rules.</p> <code>None</code> <code>train_step_fn</code> <code>Callable</code> <p>For fully customizing every training step, e.g., per-sample gradient noising for data-private training.</p> <code>None</code> Source code in <code>redco/trainers/trainer.py</code> <pre><code>def __init__(self,\n             deployer,\n             collate_fn,\n             apply_fn,\n             loss_fn,\n             params,\n             optimizer,\n             opt_state=None,\n             compute_dtype=jnp.float32,\n             last_ckpt_info=None,\n             lr_schedule_fn=None,\n             accumulate_grad_batches=None,\n             params_sharding_rules=None,\n             train_step_fn=None):\n    \"\"\"Initializes the Trainer with initial parameters, etc.\n\n    Args:\n        deployer (Deployer): A deployer supporting low-level operations.\n        collate_fn (Callable): The function converting a data batch to model\n            inputs, e.g., tokenizing sentences into input_ids.\n        apply_fn (Callable): The function to apply the model, such as\n            model.apply for Flax modules, or model itself for HuggingFace\n            models. It would be set as state.apply_fn, and used in loss_fn.\n        loss_fn (Callable): The loss function converting model inputs to a\n            scalar loss, e.g., computing cross-entropy loss from input_ids.\n        params (dict): Initial model parameters.\n        optimizer (optax optimizer): The optimizer used for training.\n        opt_state (dict): optimizer state.\n        compute_dtype (dtype): Computation dtype, e.g., `jnp.bfloat16`,\n            independent of param dtypes. (for mixed-precision training)\n        last_ckpt_info (dict): the beginning step and epoch.\n        lr_schedule_fn (Callable): The learning rate schedule\n            function converting step to learning rate.\n        accumulate_grad_batches (int): Gradient accumulation step.\n        params_sharding_rules (list): Sharding rules.\n        train_step_fn (Callable): For fully customizing every training step,\n            e.g., per-sample gradient noising for data-private training.\n    \"\"\"\n    self._deployer = deployer\n    self._collate_fn = collate_fn\n    self._apply_fn = apply_fn\n    self._loss_fn = loss_fn\n    self._optimizer = optimizer\n    self._compute_dtype = compute_dtype\n    self._lr_schedule_fn = lr_schedule_fn\n    self._accumulate_grad_batches = accumulate_grad_batches\n    self._params_sharding_rules = params_sharding_rules\n    self._train_step_fn = train_step_fn\n\n    self._state = None\n    self._state_spec = None\n    self._p_train_step = None\n    self._p_eval_step = None\n\n    self._init_step = 0\n    self._init_epoch_idx = 0\n    if last_ckpt_info is not None:\n        self._init_step = last_ckpt_info.get('step', 0)\n        self._init_epoch_idx = last_ckpt_info.get('epoch_idx', -1) + 1\n\n    n_params = sum([param.size for param in jax.tree.leaves(params)])\n    self._deployer.log_info(f'{n_params:,}', title='Parameters')\n\n    self.set_train_state(\n        apply_fn=self._apply_fn,\n        params=params,\n        optimizer=self._optimizer,\n        step=self._init_step,\n        opt_state=opt_state)\n</code></pre>"},{"location":"docs/trainer/#redco.trainers.trainer.Trainer.eval_loss","title":"<code>eval_loss(examples, per_device_batch_size, desc=None)</code>","text":"<p>Evaluates the loss on the provided examples.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>list</code> <p>Evaluation examples in list.</p> required <code>per_device_batch_size</code> <code>int</code> <p>The batch size per device.</p> required <code>desc</code> <code>str</code> <p>Description in the progress bar.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The average loss over the evaluation examples.</p> Source code in <code>redco/trainers/trainer.py</code> <pre><code>def eval_loss(self, examples, per_device_batch_size, desc=None):\n    \"\"\"Evaluates the loss on the provided examples.\n\n    Args:\n        examples (list): Evaluation examples in list.\n        per_device_batch_size (int): The batch size per device.\n        desc (str): Description in the progress bar.\n\n    Returns:\n        (float): The average loss over the evaluation examples.\n    \"\"\"\n    data_batches = self._deployer.get_model_input_batches(\n        examples=examples,\n        per_device_batch_size=per_device_batch_size,\n        collate_fn=self._collate_fn,\n        shuffle=False,\n        shuffle_rng=None,\n        desc=f'Evaluating ({desc})' if desc is not None else 'Evaluating')\n\n    losses = []\n    for batch in data_batches:\n        if self._p_eval_step is None:\n            self.setup_running_step(dummy_batch=batch)\n\n        rng = self._deployer.gen_model_step_rng()\n        metrics = self._deployer.run_model_step(\n            step_fn=self._p_eval_step, input_args=(rng, self._state, batch))\n        if self.mesh is None:\n            metrics = unreplicate(metrics)\n\n        losses.append(metrics['loss'].item())\n        data_batches.set_postfix(**metrics)\n\n    return np.mean(losses).item()\n</code></pre>"},{"location":"docs/trainer/#redco.trainers.trainer.Trainer.fit","title":"<code>fit(train_examples, per_device_batch_size, n_epochs, eval_examples=None, eval_per_device_batch_size=None, eval_loss=True, eval_predictor=None, eval_metric_fn=None, eval_sanity_check=True, save_every_ckpt=False, save_last_ckpt=False, save_argmin_ckpt_by_metrics=None, save_argmax_ckpt_by_metrics=None, save_opt_states=True, save_float_dtype=None)</code>","text":"<p>Fits the model on the training data for a given number of epochs, optionally evaluating and saving checkpoints.</p> <p>Parameters:</p> Name Type Description Default <code>train_examples</code> <code>list or Callable</code> <p>Training examples, can be a list or a function of epoch_idx (for assigning different examples in separate epochs/chunks), e.g., <code>train_examples=lambda epoch_idx: load_data(chunk_idx)</code></p> required <code>per_device_batch_size</code> <code>int</code> <p>The batch size per device.</p> required <code>n_epochs</code> <code>int</code> <p>Number of epochs to train.</p> required <code>eval_examples</code> <code>list</code> <p>Examples for evaluation and prediction.</p> <code>None</code> <code>eval_per_device_batch_size</code> <code>int</code> <p>Batch size for evaluation</p> <code>None</code> <code>eval_loss</code> <code>bool</code> <p>Whether to evaluate loss.</p> <code>True</code> <code>eval_predictor</code> <code>Predictor</code> <p>Predictor working on <code>eval_examples</code>.</p> <code>None</code> <code>eval_metric_fn</code> <code>Callable</code> <p>Metric function for prediction.</p> <code>None</code> <code>eval_sanity_check</code> <code>bool</code> <p>if to run a sanity check for evaluation &amp; predict functions before training.</p> <code>True</code> <code>save_every_ckpt</code> <code>bool</code> <p>if to save a ckpt after every epoch.</p> <code>False</code> <code>save_last_ckpt</code> <code>bool</code> <p>Whether to save the last checkpoint.</p> <code>False</code> <code>save_argmin_ckpt_by_metrics</code> <code>list[str]</code> <p>Metrics to save checkpoints based on minimum values.</p> <code>None</code> <code>save_argmax_ckpt_by_metrics</code> <code>list[str]</code> <p>Metrics to save checkpoints based on maximum values.</p> <code>None</code> <code>save_opt_states</code> <code>bool</code> <p>of to save optimizer states in ckpts.</p> <code>True</code> <code>save_float_dtype</code> <code>bool</code> <p>The data type for saving checkpoints.</p> <code>None</code> Source code in <code>redco/trainers/trainer.py</code> <pre><code>def fit(self,\n        train_examples,\n        per_device_batch_size,\n        n_epochs,\n        eval_examples=None,\n        eval_per_device_batch_size=None,\n        eval_loss=True,\n        eval_predictor=None,\n        eval_metric_fn=None,\n        eval_sanity_check=True,\n        save_every_ckpt=False,\n        save_last_ckpt=False,\n        save_argmin_ckpt_by_metrics=None,\n        save_argmax_ckpt_by_metrics=None,\n        save_opt_states=True,\n        save_float_dtype=None):\n    \"\"\"Fits the model on the training data for a given number of epochs,\n    optionally evaluating and saving checkpoints.\n\n    Args:\n        train_examples (list or Callable): Training examples, can be a\n            list or a function of epoch_idx (for assigning different\n            examples in separate epochs/chunks),\n            e.g., `train_examples=lambda epoch_idx: load_data(chunk_idx)`\n        per_device_batch_size (int): The batch size per device.\n        n_epochs (int): Number of epochs to train.\n        eval_examples (list): Examples for evaluation and prediction.\n        eval_per_device_batch_size (int): Batch size for evaluation\n        eval_loss (bool): Whether to evaluate loss.\n        eval_predictor (Predictor): Predictor working on `eval_examples`.\n        eval_metric_fn (Callable): Metric function for prediction.\n        eval_sanity_check (bool): if to run a sanity check for\n            evaluation &amp; predict functions before training.\n        save_every_ckpt (bool): if to save a ckpt after every epoch.\n        save_last_ckpt (bool): Whether to save the last checkpoint.\n        save_argmin_ckpt_by_metrics (list[str]): Metrics to save checkpoints\n            based on minimum values.\n        save_argmax_ckpt_by_metrics (list[str]): Metrics to save checkpoints\n            based on maximum values.\n        save_opt_states (bool): of to save optimizer states in ckpts.\n        save_float_dtype (bool): The data type for saving checkpoints.\n    \"\"\"\n    if eval_per_device_batch_size is None:\n        eval_per_device_batch_size = per_device_batch_size\n\n    if save_argmax_ckpt_by_metrics is None:\n        save_argmax_ckpt_by_metrics = []\n    if save_argmin_ckpt_by_metrics is None:\n        save_argmin_ckpt_by_metrics = []\n    min_metrics, max_metrics = {}, {}\n\n    if os.path.exists(f'{self.workdir}/min_metrics.json'):\n        min_metrics = json.load(open(\n            f'{self.workdir}/min_metrics.json'))\n        self._deployer.log_info(\n            json.dumps(min_metrics, indent=4), title='Detected min_metrics')\n\n    if os.path.exists(f'{self.workdir}/max_metrics.json'):\n        max_metrics = json.load(open(\n            f'{self.workdir}/max_metrics.json'))\n        self._deployer.log_info(\n            json.dumps(max_metrics, indent=4), title='Detected max_metrics')\n\n    if eval_sanity_check and eval_examples is not None:\n        rng_backup = self._deployer._rng\n        _, eval_global_micro_batch_size = \\\n            self._deployer.get_local_global_micro_batch_size(\n                per_device_batch_size=eval_per_device_batch_size)\n\n        if eval_loss:\n            self.eval_loss(\n                examples=eval_examples[:eval_global_micro_batch_size],\n                per_device_batch_size=eval_per_device_batch_size,\n                desc=f'Sanity check')\n            self._deployer.log_info(\n                'Sanity check (for evaluation loss) passed.')\n\n        if eval_predictor is not None:\n            preds = eval_predictor.predict(\n                examples=eval_examples[:eval_global_micro_batch_size],\n                params=self._state.params,\n                params_replicated=(self.mesh is None),\n                params_sharded=(self.mesh is not None),\n                per_device_batch_size=eval_per_device_batch_size,\n                desc=f'Sanity check')\n            self._deployer.log_info(\n                'Sanity check (for prediction) passed.')\n\n            if eval_metric_fn is not None:\n                json.dumps(eval_metric_fn(\n                    examples=eval_examples[:eval_global_micro_batch_size],\n                    preds=preds))\n                self._deployer.log_info(\n                    'Sanity check (for evaluation metrics) passed.')\n\n        self._deployer._rng = rng_backup\n\n    for epoch_idx in range(self._init_epoch_idx, n_epochs):\n        if isinstance(train_examples, list):\n            epoch_train_examples = train_examples\n        else:\n            epoch_train_examples = train_examples(epoch_idx=epoch_idx)\n\n        self.train(\n            examples=epoch_train_examples,\n            per_device_batch_size=per_device_batch_size,\n            desc=f'epoch {epoch_idx} / {n_epochs}')\n\n        save_ckpt_kwargs = {\n            'epoch_idx': epoch_idx,\n            'save_opt_state': save_opt_states,\n            'float_dtype': save_float_dtype\n        }\n\n        if eval_examples is None:\n            self._deployer.log_info(\n                'No evaluation cuz \\'eval_examples\\' is None.')\n        else:\n            eval_metrics = {}\n\n            if eval_loss:\n                loss = self.eval_loss(\n                    examples=eval_examples,\n                    per_device_batch_size=eval_per_device_batch_size,\n                    desc=f'epoch {epoch_idx} / {n_epochs}')\n                eval_metrics['loss'] = loss\n\n            if eval_predictor is not None:\n                preds = eval_predictor.predict(\n                    examples=eval_examples,\n                    params=self._state.params,\n                    params_replicated=(self.mesh is None),\n                    params_sharded=(self.mesh is not None),\n                    per_device_batch_size=eval_per_device_batch_size,\n                    desc=f'epoch {epoch_idx} / {n_epochs}')\n\n                if eval_metric_fn is not None:\n                    eval_metrics.update(eval_metric_fn(\n                        examples=eval_examples, preds=preds))\n\n                eval_outputs = [\n                    {'example': example, 'pred': pred}\n                    for example, pred in zip(eval_examples, preds)]\n\n                self._deployer.save_outputs(\n                    outputs=eval_outputs,\n                    desc=f'epoch{epoch_idx}',\n                    step=self.step)\n\n            self._deployer.log_info(\n                info=json.dumps(eval_metrics, indent=4),\n                title=f'Eval results',\n                step=self.step)\n            self._deployer.log_metrics(metrics={\n                f'eval_{key}': value\n                for key, value in eval_metrics.items()\n            }, step=self.step)\n\n            if self.workdir is not None:\n                result_filepath = \\\n                    f'{self.workdir}/eval_results_epoch{epoch_idx}.json'\n                json.dump(\n                    eval_metrics, open(result_filepath, 'w'), indent=4)\n                self._deployer.log_info(\n                    f'eval_results saved into {result_filepath}.')\n\n            for key in save_argmin_ckpt_by_metrics:\n                assert self.workdir is not None\n                if eval_metrics[key] &lt; min_metrics.get(key, float('inf')):\n                    min_metrics[key] = eval_metrics[key]\n\n                    if jax.process_index() == 0:\n                        self._deployer.log_info(\n                            f'minimal {key} updated to {min_metrics[key]}')\n                        json.dump(min_metrics, open(\n                            f'{self.workdir}/min_metrics.json', 'w'))\n\n                    self.save_ckpt(\n                        ckpt_name=f'min_{key}', **save_ckpt_kwargs)\n\n            for key in save_argmax_ckpt_by_metrics:\n                assert self.workdir is not None\n                if eval_metrics[key] &gt; max_metrics.get(key, float('-inf')):\n                    max_metrics[key] = eval_metrics[key]\n\n                    if jax.process_index() == 0:\n                        self._deployer.log_info(\n                            f'maximal {key} updated to {max_metrics[key]}')\n                        json.dump(max_metrics, open(\n                            f'{self.workdir}/max_metrics.json', 'w'))\n\n                    self.save_ckpt(\n                        ckpt_name=f'max_{key}', **save_ckpt_kwargs)\n\n        if save_every_ckpt:\n            self.save_ckpt(\n                ckpt_name=f'epoch_{epoch_idx}', **save_ckpt_kwargs)\n        elif save_last_ckpt:\n            self.save_ckpt(ckpt_name='last', **save_ckpt_kwargs)\n</code></pre>"},{"location":"docs/trainer/#redco.trainers.trainer.Trainer.save_ckpt","title":"<code>save_ckpt(epoch_idx, ckpt_name, save_opt_state, float_dtype)</code>","text":"<p>Saves a checkpoint into <code>{self.workdir}/ckpts</code>.</p> <p>Parameters:</p> Name Type Description Default <code>epoch_idx</code> <code>int</code> <p>The current epoch index.</p> required <code>ckpt_name</code> <code>str</code> <p>The name of the checkpoint.</p> required <code>save_opt_state</code> <code>bool</code> <p>Whether to save the optimizer state.</p> required <code>float_dtype</code> <code>`jax.numpy.dtype`</code> <p>Data type for saving float params.</p> required Source code in <code>redco/trainers/trainer.py</code> <pre><code>def save_ckpt(self, epoch_idx, ckpt_name, save_opt_state, float_dtype):\n    \"\"\"Saves a checkpoint into `{self.workdir}/ckpts`.\n\n    Args:\n        epoch_idx (int): The current epoch index.\n        ckpt_name (str): The name of the checkpoint.\n        save_opt_state (bool): Whether to save the optimizer state.\n        float_dtype (`jax.numpy.dtype`): Data type for saving float params.\n    \"\"\"\n    if self.mesh is None:\n        params = jax.tree.map(\n            fully_replicated_host_local_array_to_global_array,\n            self._state.params)\n    else:\n        params = self._state.params\n\n    opt_state = None\n    if save_opt_state:\n        if self.mesh is None:\n            opt_state = jax.tree.map(\n                fully_replicated_host_local_array_to_global_array,\n                self._state.opt_state)\n        else:\n            opt_state = self._state.opt_state\n\n    ckpt_dir = f'{self.workdir}/ckpts/{ckpt_name}'\n    self._deployer.save_ckpt(\n        ckpt_dir=ckpt_dir,\n        params=params,\n        opt_state=opt_state,\n        float_dtype=float_dtype,\n        step=self.step,\n        epoch_idx=epoch_idx)\n\n    if jax.process_index() == 0:\n        open(f'{self.workdir}/ckpts/last_ckpt.txt', 'w').write(ckpt_name)\n        self._deployer.log_info(f'last ckpt updated -- {ckpt_dir}')\n</code></pre>"},{"location":"docs/trainer/#redco.trainers.trainer.Trainer.set_train_state","title":"<code>set_train_state(apply_fn, params, optimizer, step, opt_state=None)</code>","text":"<p>Sets/Resets the training state with given parameters and optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>apply_fn</code> <code>Callable</code> <p>The function to apply the model.</p> required <code>params</code> <code>dict</code> <p>Model parameters.</p> required <code>optimizer</code> <code>dict</code> <p>The optimizer used for training.</p> required <code>step</code> <code>int</code> <p>The training step.</p> required <code>opt_state</code> <code>dict</code> <p>The state of the optimizer.</p> <code>None</code> Source code in <code>redco/trainers/trainer.py</code> <pre><code>def set_train_state(\n        self, apply_fn, params, optimizer, step, opt_state=None):\n    \"\"\"Sets/Resets the training state with given parameters and optimizer.\n\n    Args:\n        apply_fn (Callable): The function to apply the model.\n        params (dict): Model parameters.\n        optimizer (dict): The optimizer used for training.\n        step (int): The training step.\n        opt_state (dict): The state of the optimizer.\n    \"\"\"\n    self._deployer.log_info('Setting train_state ...')\n    params = freeze(params)\n\n    if self.mesh is None:\n        params = jax.device_put(params, jax.local_devices()[0])\n        if opt_state is None:\n            self._deployer.log_info('Initializing opt_state ...')\n            opt_state = optimizer.init(params)\n        else:\n            opt_state = jax.device_put(opt_state, jax.local_devices()[0])\n\n        self._state = train_state.TrainState(\n            step=step,\n            apply_fn=apply_fn,\n            params=params,\n            tx=optimizer,\n            opt_state=opt_state)\n        self._state = replicate(self._state)\n    else:\n        params_spec = self._deployer.get_params_spec(\n            params_shape_or_params=params,\n            params_sharding_rules=self._params_sharding_rules)\n        params = self._deployer.shard_params(\n            params=params, params_spec=params_spec)\n\n        if opt_state is None:\n            self._deployer.log_info('Initializing opt_state ...')\n            opt_state = optimizer.init(params)\n\n        opt_state_spec = self._deployer.get_opt_state_spec(\n            params_shape_or_params=params,\n            params_spec=params_spec,\n            optimizer=optimizer)\n        opt_state = self._deployer.shard_params(\n            params=opt_state,\n            params_spec=opt_state_spec,\n            desc='opt_state')\n\n        self._state = train_state.TrainState(\n            apply_fn=apply_fn,\n            params=params,\n            tx=optimizer,\n            opt_state=opt_state,\n            step=step)\n\n        self._state_spec = train_state.TrainState(\n            apply_fn=apply_fn,\n            params=params_spec,\n            tx=optimizer,\n            opt_state=opt_state_spec,\n            step=None)\n</code></pre>"},{"location":"docs/trainer/#redco.trainers.trainer.Trainer.setup_running_step","title":"<code>setup_running_step(dummy_batch)</code>","text":"<p>Sets up the running step functions for training and evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>dummy_batch</code> <code>PyTree</code> <p>A dummy batch of data.</p> required Source code in <code>redco/trainers/trainer.py</code> <pre><code>def setup_running_step(self, dummy_batch):\n    \"\"\"Sets up the running step functions for training and evaluation.\n\n    Args:\n        dummy_batch (PyTree): A dummy batch of data.\n    \"\"\"\n    train_step_fn = partial(\n        self._train_step_fn or default_train_step,\n        loss_fn=self._loss_fn,\n        lr_schedule_fn=self._lr_schedule_fn,\n        mesh=self.mesh,\n        compute_dtype=self._compute_dtype)\n    eval_step_fn = partial(\n        eval_step,\n        loss_fn=self._loss_fn,\n        mesh=self.mesh,\n        compute_dtype=self._compute_dtype)\n\n    if self.mesh is None:\n        self._p_train_step = jax.pmap(train_step_fn, axis_name='dp')\n        self._p_eval_step = jax.pmap(eval_step_fn, axis_name='dp')\n    else:\n        data_spec = jax.tree.map(lambda x: P('dp'), dummy_batch)\n        self._p_train_step = pjit(\n            train_step_fn,\n            in_shardings=(None, self._state_spec, data_spec),\n            out_shardings=(self._state_spec, None),\n            donate_argnums=(1, ))\n        self._p_eval_step = pjit(\n            eval_step_fn,\n            in_shardings=(None, self._state_spec, data_spec),\n            out_shardings=None)\n</code></pre>"},{"location":"docs/trainer/#redco.trainers.trainer.Trainer.train","title":"<code>train(examples, per_device_batch_size, desc=None)</code>","text":"<p>Trains the model on the provided examples.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>list</code> <p>Training examples in python list.</p> required <code>per_device_batch_size</code> <code>int</code> <p>The batch size per device.</p> required <code>desc</code> <code>str</code> <p>Description in the progress bar.</p> <code>None</code> Source code in <code>redco/trainers/trainer.py</code> <pre><code>def train(self, examples, per_device_batch_size, desc=None):\n    \"\"\"Trains the model on the provided examples.\n\n    Args:\n        examples (list): Training examples in python list.\n        per_device_batch_size (int): The batch size per device.\n        desc (str): Description in the progress bar.\n    \"\"\"\n    data_batches = self._deployer.get_model_input_batches(\n        examples=examples,\n        per_device_batch_size=per_device_batch_size,\n        collate_fn=self._collate_fn,\n        shuffle=True,\n        shuffle_rng=self._deployer.gen_rng(),\n        desc=f'Training ({desc})' if desc is not None else 'Training',\n        is_train=True,\n        accumulate_grad_batches=self._accumulate_grad_batches)\n\n    for batch in data_batches:\n        if self._p_train_step is None:\n            self.setup_running_step(dummy_batch=batch)\n\n        rng = self._deployer.gen_model_step_rng()\n        self._state, metrics = self._deployer.run_model_step(\n            step_fn=self._p_train_step,\n            input_args=(rng, self._state, batch))\n\n        if self.mesh is None:\n            metrics = unreplicate(metrics)\n        data_batches.set_postfix(**metrics)\n        self._deployer.log_metrics(metrics=metrics, step=self.step)\n</code></pre>"},{"location":"tutorial/mnist_dp/","title":"MNIST - Data Parallel Only","text":"<p>This is a trivial MNIST example with RedCoast (<code>pip install redco==0.4.22</code>). Runnable by  <pre><code>python main.py\n</code></pre> This example supports data parallelism only, see the model parallel example for model parallelism with one more argument. </p> <p>To simulate multiple devices in cpu-only envs, <pre><code>XLA_FLAGS=\"--xla_force_host_platform_device_count=8\" python main.py\n</code></pre></p>"},{"location":"tutorial/mnist_dp/#source-code","title":"Source Code","text":"<pre><code>from functools import partial\nimport fire\nimport numpy as np\nfrom flax import linen as nn\nimport optax\nfrom torchvision.datasets import MNIST\nfrom redco import Deployer, Trainer, Predictor\n\n\n# A simple CNN model \n# Copied from https://github.com/google/flax/blob/main/examples/mnist/train.py\nclass CNN(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n        x = x.reshape((x.shape[0], -1))  # flatten\n        x = nn.Dense(features=256)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=10)(x)\n        return x\n\n\n# Collate function converting a batch of raw examples to model inputs (in numpy) \ndef collate_fn(examples):\n    images = np.stack(\n        [np.array(example['image'])[:, :, None] for example in examples])\n    labels = np.array([example['label'] for example in examples])\n\n    return {'images': images, 'labels': labels}\n\n\n# Loss function converting model inputs to a scalar loss\ndef loss_fn(rng, state, params, batch, is_training):\n    logits = state.apply_fn({'params': params}, batch['images'])\n    return optax.softmax_cross_entropy_with_integer_labels(\n        logits=logits, labels=batch['labels']).mean()\n\n\n# Predict function converting model inputs to the model outputs\ndef pred_fn(rng, params, batch, model):\n    return model.apply({'params': params}, batch['images']).argmax(axis=-1)\n\n\n# (Optional) Evaluation function in trainer.fit. Here it computes accuracy.\ndef eval_metric_fn(examples, preds):\n    preds = np.array(preds)\n    labels = np.array([example['label'] for example in examples])\n    return {'acc': np.mean(preds == labels).item()}\n\n\ndef main(per_device_batch_size=64, learning_rate=1e-3, jax_seed=42):\n    deployer = Deployer(jax_seed=jax_seed, workdir='./workdir')\n\n    dataset = {\n        'train': [{'image': t[0], 'label': t[1]} for t in list(\n            MNIST('./data', train=True, download=True))],\n        'test': [{'image': t[0], 'label': t[1]} for t in list(\n            MNIST('./data', train=False, download=True))],\n    }\n\n    model = CNN()\n    dummy_batch = collate_fn(examples=[dataset['train'][0]])\n    params = model.init(deployer.gen_rng(), dummy_batch['images'])['params']\n\n    trainer = Trainer(\n        deployer=deployer,\n        collate_fn=collate_fn,\n        apply_fn=model.apply,\n        loss_fn=loss_fn,\n        params=params,\n        optimizer=optax.adamw(learning_rate=learning_rate))\n\n    predictor = Predictor(\n        deployer=deployer,\n        collate_fn=collate_fn,\n        pred_fn=partial(pred_fn, model=model))\n\n    trainer.fit(\n        train_examples=dataset['train'],\n        per_device_batch_size=per_device_batch_size,\n        n_epochs=2,\n        eval_examples=dataset['test'],\n        eval_predictor=predictor,\n        eval_metric_fn=eval_metric_fn)\n\n\nif __name__ == '__main__':\n    fire.Fire(main)\n</code></pre>"},{"location":"tutorial/mnist_mp/","title":"MNIST - Model Parallel","text":"<p>This is a MNIST example with RedCoast (<code>pip install redco==0.4.22</code>), supporting model parallelism by passing in <code>n_model_shards</code> to <code>redco.Deployer</code>.</p> <p>To simulate multiple devices in cpu-only envs, <pre><code>XLA_FLAGS=\"--xla_force_host_platform_device_count=8\" python main.py --n_model_shards 4\n</code></pre></p>"},{"location":"tutorial/mnist_mp/#source-code","title":"Source Code","text":"<pre><code>from functools import partial\nimport fire\nimport numpy as np\nfrom flax import linen as nn\nimport optax\nfrom torchvision.datasets import MNIST\nfrom redco import Deployer, Trainer, Predictor\n\n\n# A simple CNN model \n# Copied from https://github.com/google/flax/blob/main/examples/mnist/train.py\nclass CNN(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n        x = x.reshape((x.shape[0], -1))  # flatten\n        x = nn.Dense(features=256)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=10)(x)\n        return x\n\n\n# Collate function converting a batch of raw examples to model inputs (in numpy) \ndef collate_fn(examples):\n    images = np.stack(\n        [np.array(example['image'])[:, :, None] for example in examples])\n    labels = np.array([example['label'] for example in examples])\n\n    return {'images': images, 'labels': labels}\n\n\n# Loss function converting model inputs to a scalar loss\ndef loss_fn(rng, state, params, batch, is_training):\n    logits = state.apply_fn({'params': params}, batch['images'])\n    return optax.softmax_cross_entropy_with_integer_labels(\n        logits=logits, labels=batch['labels']).mean()\n\n\n# Predict function converting model inputs to the model outputs\ndef pred_fn(rng, params, batch, model):\n    return model.apply({'params': params}, batch['images']).argmax(axis=-1)\n\n\n# (Optional) Evaluation function in trainer.fit. Here it computes accuracy.\ndef eval_metric_fn(examples, preds):\n    preds = np.array(preds)\n    labels = np.array([example['label'] for example in examples])\n    return {'acc': np.mean(preds == labels).item()}\n\n\ndef main(per_device_batch_size=64,\n         learning_rate=1e-3,\n         jax_seed=42,\n         n_model_shards=2):\n    deployer = Deployer(\n        jax_seed=jax_seed, workdir='./workdir', n_model_shards=n_model_shards)\n\n    dataset = {\n        'train': [{'image': t[0], 'label': t[1]} for t in list(\n            MNIST('./data', train=True, download=True))],\n        'test': [{'image': t[0], 'label': t[1]} for t in list(\n            MNIST('./data', train=False, download=True))],\n    }\n\n    model = CNN()\n    dummy_batch = collate_fn(examples=[dataset['train'][0]])\n    params = model.init(deployer.gen_rng(), dummy_batch['images'])['params']\n\n    # automatically generate sharding rules. can be adjusted before passing in \n    # to Trainer/Predictor if you feel it's not potimal\n    params_sharding_rules = deployer.get_sharding_rules(\n        params_shape_or_params=params)\n    if params_sharding_rules is not None:\n        deployer.log_info(\n            info='\\n'.join([str(t) for t in params_sharding_rules]),\n            title='Sharding rules')\n\n    trainer = Trainer(\n        deployer=deployer,\n        collate_fn=collate_fn,\n        apply_fn=model.apply,\n        loss_fn=loss_fn,\n        params=params,\n        optimizer=optax.adamw(learning_rate=learning_rate),\n        params_sharding_rules=params_sharding_rules)\n\n    predictor = Predictor(\n        deployer=deployer,\n        collate_fn=collate_fn,\n        pred_fn=partial(pred_fn, model=model),\n        params_sharding_rules=params_sharding_rules)\n\n    trainer.fit(\n        train_examples=dataset['train'],\n        per_device_batch_size=per_device_batch_size,\n        n_epochs=2,\n        eval_examples=dataset['test'],\n        eval_predictor=predictor,\n        eval_metric_fn=eval_metric_fn)\n\n\nif __name__ == '__main__':\n    fire.Fire(main)\n</code></pre>"},{"location":"tutorial/mnist_private/","title":"MNIST - Differential-Private","text":"<p>This is a MNIST example with RedCoast (<code>pip install redco==0.4.22</code>), supporting differentially-private training  <pre><code>python main.py --noise_multiplier 1.\n</code></pre></p> <p>To simulate multiple devices in cpu-only envs, <pre><code>XLA_FLAGS=\"--xla_force_host_platform_device_count=8\" python main.py --noise_multiplier 1.\n</code></pre></p>"},{"location":"tutorial/mnist_private/#source-code","title":"Source Code","text":"<pre><code>from functools import partial\nimport fire\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nfrom jax.example_libraries.optimizers import l2_norm\nfrom flax import linen as nn\nimport optax\nfrom torchvision.datasets import MNIST\nfrom redco import Deployer, Trainer, Predictor\n\n\n# A simple CNN model\n# Copied from https://github.com/google/flax/blob/main/examples/mnist/train.py\nclass CNN(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n        x = x.reshape((x.shape[0], -1))  # flatten\n        x = nn.Dense(features=256)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=10)(x)\n        return x\n\n\n# Collate function converting a batch of raw examples to model inputs (in numpy)\ndef collate_fn(examples):\n    images = np.stack(\n        [np.array(example['image'])[:, :, None] for example in examples])\n    labels = np.array([example['label'] for example in examples])\n\n    return {'images': images, 'labels': labels}\n\n\n# Loss function converting model inputs to a scalar loss\ndef loss_fn(rng, state, params, batch, is_training):\n    logits = state.apply_fn({'params': params}, batch['images'])\n    return optax.softmax_cross_entropy_with_integer_labels(\n        logits=logits, labels=batch['labels']).mean()\n\n\n# Predict function converting model inputs to the model outputs\ndef pred_fn(rng, params, batch, model):\n    return model.apply({'params': params}, batch['images']).argmax(axis=-1)\n\n\n# (Optional) Evaluation function in trainer.fit. Here it computes accuracy.\ndef eval_metric_fn(examples, preds):\n    preds = np.array(preds)\n    labels = np.array([example['label'] for example in examples])\n    return {'acc': np.mean(preds == labels).item()}\n\n\n# adapted from default_train_step(), added `loss_and_per_sample_grads`\n# https://github.com/tanyuqian/redco/blob/master/redco/trainers/utils.py\ndef dp_train_step(\n        rng, state, batch, loss_fn, lr_schedule_fn, mesh, compute_dtype):\n    def loss_and_grads(rng_, batch_):\n        return jax.value_and_grad(\n            lambda params: loss_fn(\n                rng=rng_,\n                state=state,\n                params=params,\n                batch=batch_,\n                is_training=True)\n        )(jax.tree.map(lambda x: x.astype(compute_dtype), state.params))\n\n    def loss_and_per_sample_grads(rng_, batch_):\n        batch_ = jax.tree.map(lambda x: x[:, None], batch_)\n        loss, grads = jax.vmap(lambda b: loss_and_grads(rng_, b))(batch_)\n\n        return loss.mean(), grads\n\n    if mesh is None:\n        loss, grads = loss_and_per_sample_grads(rng, batch)\n        grads = jax.lax.pmean(grads, axis_name='dp')\n    else:\n        loss, grads = jax.vmap(loss_and_per_sample_grads)(\n            jax.random.split(rng, num=mesh.shape['dp']), batch)\n        loss = jnp.mean(loss)\n        grads = jax.tree.map(lambda x: jnp.mean(x, axis=0), grads)\n\n    new_state = state.apply_gradients(grads=jax.tree.map(\n        lambda grad, param: grad.astype(param.dtype), grads, state.params))\n\n    metrics = {'loss': loss, 'step': state.step, 'grad_norm': l2_norm(grads)}\n    if lr_schedule_fn is not None:\n        metrics['lr'] = lr_schedule_fn(state.step)\n    if mesh is None:\n        metrics = jax.lax.pmean(metrics, axis_name='dp')\n\n    return new_state, metrics\n\n\ndef main(per_device_batch_size=64,\n         learning_rate=1e-3,\n         jax_seed=42,\n         noise_multiplier=1.):\n    deployer = Deployer(jax_seed=jax_seed, workdir='./workdir')\n\n    dataset = {\n        'train': [{'image': t[0], 'label': t[1]} for t in list(\n            MNIST('./data', train=True, download=True))],\n        'test': [{'image': t[0], 'label': t[1]} for t in list(\n            MNIST('./data', train=False, download=True))],\n    }\n\n    model = CNN()\n    dummy_batch = collate_fn(examples=[dataset['train'][0]])\n    params = model.init(deployer.gen_rng(), dummy_batch['images'])['params']\n\n    optimizer = optax.chain(\n        optax.contrib.differentially_private_aggregate(\n            l2_norm_clip=1.,\n            noise_multiplier=noise_multiplier,\n            seed=jax_seed),\n        optax.adamw(learning_rate=learning_rate)\n    )\n\n    trainer = Trainer(\n        deployer=deployer,\n        collate_fn=collate_fn,\n        apply_fn=model.apply,\n        loss_fn=loss_fn,\n        params=params,\n        optimizer=optimizer,\n        train_step_fn=dp_train_step)\n\n    predictor = Predictor(\n        deployer=deployer,\n        collate_fn=collate_fn,\n        pred_fn=partial(pred_fn, model=model))\n\n    trainer.fit(\n        train_examples=dataset['train'],\n        per_device_batch_size=per_device_batch_size,\n        n_epochs=2,\n        eval_examples=dataset['test'],\n        eval_predictor=predictor,\n        eval_metric_fn=eval_metric_fn)\n\n\nif __name__ == '__main__':\n    fire.Fire(main)\n</code></pre>"}]}